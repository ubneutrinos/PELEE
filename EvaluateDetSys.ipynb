{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import localSettings as ls\n",
    "import os\n",
    "print(ls.main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1560556807118,
     "user": {
      "displayName": "Stefano Roberto Soleti",
      "photoUrl": "https://lh4.googleusercontent.com/-hfLpspJu4Q0/AAAAAAAAAAI/AAAAAAAABmA/2kE4rtj8paU/s64/photo.jpg",
      "userId": "10372352518008961760"
     },
     "user_tz": 240
    },
    "id": "6qsD0G-yYJ9K",
    "outputId": "5d52a3ec-50be-44fc-da44-3c0593e98bc6"
   },
   "outputs": [],
   "source": [
    "main_path = ls.main_path\n",
    "sys.path.append(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m%d%Y\")\n",
    "print(\"date and time:\",date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gauss(x,mu,sigma,A):\n",
    "    norm = A/(np.sqrt(2*np.pi)*sigma)\n",
    "    exp  = np.exp(-((x-mu)**2)/(2*sigma*sigma))\n",
    "    return norm * exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGqTJ5JgaDsx"
   },
   "outputs": [],
   "source": [
    "import plotter\n",
    "import importlib\n",
    "importlib.reload(plotter)\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "#if USEBDT:\n",
    "import xgboost as xgb\n",
    "import nue_booster \n",
    "importlib.reload(nue_booster)\n",
    "import awkward\n",
    "import pandas as pd\n",
    "\n",
    "params = {\n",
    "    'axes.labelsize': 'x-large',\n",
    "    'axes.titlesize': 'x-large',\n",
    "    'xtick.labelsize': 'x-large',\n",
    "    'ytick.labelsize': 'x-large'\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_data_run123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COVARIANCE(n_cv,n_var):\n",
    "    cov = np.empty([len(n_cv), len(n_cv)])\n",
    "    cov.fill(0)\n",
    "\n",
    "    for i in range(len(n_cv)):\n",
    "        for j in range(len(n_cv)):\n",
    "            cov[i][j] += (n_var[i] - n_cv[i])*(n_var[j] - n_cv[j])\n",
    "\n",
    "    frac_cov = np.empty([len(n_cv), len(n_cv)])\n",
    "    corr = np.empty([len(n_cv), len(n_cv)])\n",
    "\n",
    "    for i in range(len(n_cv)):\n",
    "        for j in range(len(n_cv)):\n",
    "            frac_cov[i][j] =  cov[i][j] / (n_cv[i] * n_cv[j])\n",
    "            corr[i][j] = cov[i][j] / np.sqrt(cov[i][i] * cov[j][j])\n",
    "    return cov,frac_cov,corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINVAR = [\"shr_score\",\"tksh_distance\",\"tksh_angle\",\n",
    "            \"shr_tkfit_dedx_max\",\n",
    "            \"trkfit\",\"trkpid\",\n",
    "            \"subcluster\",\"shrmoliereavg\",\n",
    "            \"trkshrhitdist2\",\"hits_ratio\",\n",
    "            \"secondshower_Y_nhit\",\"secondshower_Y_vtxdist\",\"secondshower_Y_dot\",\"anglediff_Y\",\n",
    "            \"CosmicIPAll3D\",\"CosmicDirAll3D\"]\n",
    "\n",
    "LABELS =  ['pi0','nonpi0']\n",
    "\n",
    "TRAINVARZP = ['shrmoliereavg','shr_score', \"trkfit\",\"subcluster\",\n",
    "              \"CosmicIPAll3D\",\"CosmicDirAll3D\",\n",
    "              'secondshower_Y_nhit','secondshower_Y_vtxdist','secondshower_Y_dot','anglediff_Y',\n",
    "              'secondshower_V_nhit','secondshower_V_vtxdist','secondshower_V_dot','anglediff_V',\n",
    "              'secondshower_U_nhit','secondshower_U_vtxdist','secondshower_U_dot','anglediff_U',\n",
    "              \"shr_tkfit_2cm_dedx_U\", \"shr_tkfit_2cm_dedx_V\", \"shr_tkfit_2cm_dedx_Y\",\n",
    "              \"shr_tkfit_gap10_dedx_U\", \"shr_tkfit_gap10_dedx_V\", \"shr_tkfit_gap10_dedx_Y\",\n",
    "              \"shrMCSMom\",\"DeltaRMS2h\",\"shrPCA1CMed_5cm\",\"CylFrac2h_1cm\"]\n",
    "\n",
    "LABELSZP = ['bkg']\n",
    "\n",
    "def loadBDT(DF):\n",
    "\n",
    "    for label, bkg_query in zip(LABELS, nue_booster.bkg_queries):\n",
    "        with open(ls.pickle_path+'booster_%s_0304_extnumi.pickle' % label, 'rb') as booster_file:\n",
    "            booster = pickle.load(booster_file)\n",
    "            DF[label+\"_score\"] = booster.predict(xgb.DMatrix(DF[TRAINVAR]),\n",
    "                                                 ntree_limit=booster.best_iteration)\n",
    "\n",
    "    for label, bkg_query in zip(LABELSZP, nue_booster.bkg_queries):\n",
    "        with open(ls.pickle_path+'booster_%s_0304_extnumi_vx.pickle' % label, 'rb') as booster_file:\n",
    "            booster = pickle.load(booster_file)\n",
    "            DF[label+\"_score\"] = booster.predict(xgb.DMatrix(DF[TRAINVARZP]),\n",
    "                                                 ntree_limit=booster.best_iteration)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadnuevars(df):\n",
    "    \n",
    "    INTERCEPT = 0.0\n",
    "    SLOPE = 0.83\n",
    "\n",
    "    if (loadnues):\n",
    "        df['subcluster'] = df['shrsubclusters0'] + df['shrsubclusters1'] + df['shrsubclusters2']\n",
    "        df['trkfit'] = df['shr_tkfit_npointsvalid'] / df['shr_tkfit_npoints']\n",
    "        df['anglediff_Y'] = np.abs(df['secondshower_Y_dir']-df['shrclusdir2'])\n",
    "        df['anglediff_V'] = np.abs(df['secondshower_V_dir']-df['shrclusdir1'])\n",
    "        df['anglediff_U'] = np.abs(df['secondshower_U_dir']-df['shrclusdir0'])                                                                                                                       \n",
    "        df[\"ptOverP\"] = df[\"pt\"]/df[\"p\"]\n",
    "        df[\"phi1MinusPhi2\"] = df[\"shr_phi\"]-df[\"trk_phi\"]\n",
    "        df[\"theta1PlusTheta2\"] = df[\"shr_theta\"]+df[\"trk_theta\"]\n",
    "        df['cos_shr_theta'] = np.cos(df['shr_theta'])\n",
    "        df['shr_tkfit_nhits_tot'] = (df['shr_tkfit_nhits_Y']+df['shr_tkfit_nhits_U']+df['shr_tkfit_nhits_V'])                                                                                                                                                                                              \n",
    "        df['shr_tkfit_2cm_nhits_tot'] = (df['shr_tkfit_2cm_nhits_Y']+df['shr_tkfit_2cm_nhits_U']+df['shr_tkfit_2cm_nhits_V'])                                                                                                                                                              \n",
    "        df['shr_tkfit_gap10_nhits_tot'] = (df['shr_tkfit_gap10_nhits_Y']+df['shr_tkfit_gap10_nhits_U']+df['shr_tkfit_gap10_nhits_V'])                                                                                                                             \n",
    "        df.loc[:,'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_Y']\n",
    "        df.loc[(df['shr_tkfit_nhits_U']>df['shr_tkfit_nhits_Y']),'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_U']\n",
    "        df.loc[(df['shr_tkfit_nhits_V']>df['shr_tkfit_nhits_Y']) & (df['shr_tkfit_nhits_V']>df['shr_tkfit_nhits_U']),'shr_tkfit_dedx_max'] = df['shr_tkfit_dedx_V']\n",
    "        df[\"reco_e\"] = (df[\"shr_energy_tot_cali\"] + INTERCEPT) / SLOPE + df[\"trk_energy_tot\"]\n",
    "        df['electron_e'] = (df[\"shr_energy_tot_cali\"] + INTERCEPT) / SLOPE\n",
    "\n",
    "\n",
    "        loadBDT(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadnues = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nue variables\n",
    "\n",
    "nueCV   = 'nue_CV.root'\n",
    "nueLYD  = 'nue_LYDown.root'\n",
    "nueLYR  = 'nue_LYRayleigh.root'\n",
    "nueLYA  = 'nue_LYAttenuation.root'\n",
    "nueX    = 'nue_ScaleX.root'\n",
    "nueYZ   = 'nue_ScaleYZ.root'\n",
    "nueAXZ  = 'nue_AngleXZ.root'\n",
    "nueAYZ  = 'nue_AngleYZ.root'\n",
    "nueSCE  = 'nue_SCE.root'\n",
    "nuedEdX = 'nue_dEdX.root'\n",
    "\n",
    "PATH = '/home/david/data/searchingfornues/v08_00_00_44/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DETVAR_N_V = [\"X\", \"YZ\", 'aYZ', \"aXZ\"]#,\"dEdX\",\"SCE\",\"LYD\",\"LYR\",\"LYA\"]\n",
    "#DETVAR_S_V = [nueX,nueYZ,nueAYZ,nueAXZ]#,nuedEdX,nueSCE,nueLYD,nueLYR,nueLYA]\n",
    "\n",
    "DETVAR_N_V = [\"dEdX\",\"SCE\"]\n",
    "DETVAR_S_V = [nuedEdX,nueSCE]\n",
    "\n",
    "#DETVAR_N_V = [\"LYD\",\"LYR\",\"LYA\"]\n",
    "#DETVAR_S_V = [nueLYD,nueLYR,nueLYA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'nuselection'\n",
    "tree = 'NeutrinoSelectionFilter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARDICT = load_data_run123.get_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (loadnues):\n",
    "    # nues\n",
    "    variables = VARDICT['VARIABLES'] + VARDICT['NUEVARS'] #+ load_data_run123.RCVRYVARS\n",
    "else:\n",
    "    # numus\n",
    "    variables = load_data_run123.VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Value DF\n",
    "CV = uproot.open(PATH+nueCV)[fold][tree]\n",
    "CVDF  = CV.pandas.df(variables, flatten=False)\n",
    "\n",
    "load_data_run123.process_uproot(CV,CVDF)\n",
    "if (loadnues):\n",
    "    loadnuevars(CVDF)\n",
    "\n",
    "CVDF['bnbdata']  = np.zeros_like(CVDF['nslice'])\n",
    "CVDF['extdata']  = np.zeros_like(CVDF['nslice'])\n",
    "\n",
    "\n",
    "CVDF['identifier'] = CVDF['run']*100000 + CVDF['evt']  + ((100.*CVDF['nu_e']).astype(int))/1000. #to line up events with sample events\n",
    "\n",
    "NCV = CVDF.shape[0]\n",
    "\n",
    "print ('there are %i CV events'%(CVDF.shape[0]))\n",
    "\n",
    "DETSYS_SAMPLE_V = [] #list of merged CV-VAR dfataframes\n",
    "POT_V = [] #POT of samples? Not really used elsewhere\n",
    "\n",
    "for i,N in enumerate(DETVAR_N_V):\n",
    "    \n",
    "    VAR = uproot.open(PATH+DETVAR_S_V[i])[fold][tree]\n",
    "    VARDF = VAR.pandas.df(variables, flatten=False)\n",
    "    \n",
    "    VARDF['bnbdata']  = np.zeros_like(VARDF['nslice'])\n",
    "    VARDF['extdata']  = np.zeros_like(VARDF['nslice'])\n",
    "\n",
    "    load_data_run123.process_uproot(VAR,VARDF)\n",
    "    if (loadnues):\n",
    "        loadnuevars(VARDF)\n",
    "\n",
    "    VARDF['identifier'] = VARDF['run']*100000 + VARDF['evt']  + ((100.*VARDF['nu_e']).astype(int))/1000.     \n",
    "    INT = pd.merge(CVDF, VARDF, how='inner', on=['identifier'],suffixes=('_CV', '_VAR'))\n",
    "\n",
    "    print ('intersection for %15s variation has %i events'%(DETVAR_N_V[i],INT.shape[0]))\n",
    "    DETSYS_SAMPLE_V.append(INT)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _selection(variable, sample, query=\"selected==1\"):#, extra_cut='',verbose=False):\n",
    "    \"\"\"\n",
    "    variable: string, about which info to be returned\n",
    "    sample: dataframe, to be queried\n",
    "    query: string, event-level queries\n",
    "    extra_cut: string, just another cut in addition to query\n",
    "    select_longest: bool, will select longest track in each slice after cuts\n",
    "        should be on when variable is track-level\n",
    "    fix: string, should be \"_CV\" or \"_VAR\"\n",
    "        on which sample to apply the cuts\n",
    "    return_fix: string, should be \"_CV\" or \"_VAR\"\n",
    "        which sample to return\n",
    "    \n",
    "    Returns an array of track/event variables that pass cuts\n",
    "    \"\"\"\n",
    "        \n",
    "    df = sample.copy().query(query)\n",
    "    #start dealing with the track variables\n",
    "    VARS = df[variable]#VARS is not clean of empty frames\n",
    "    \n",
    "    return VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "import unblinding_far_sideband\n",
    "\n",
    "import detsysselections\n",
    "importlib.reload(detsysselections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdb_key = 'None'\n",
    "pre_key = 'NP'\n",
    "sel_key = 'NPBDT'\n",
    "\n",
    "QUERY_CV  = detsysselections.BDTCQ(\"CV\")\n",
    "QUERY_VAR = detsysselections.BDTCQ(\"VAR\")\n",
    "\n",
    "sideband = unblinding_far_sideband.sideband_categories[sdb_key]\n",
    "preselection = unblinding_far_sideband.preselection_categories[pre_key]\n",
    "sel =  unblinding_far_sideband.selection_categories[sel_key]\n",
    "\n",
    "SAVEPATH = ls.main_path+'/detsys/'+'{}_{}_{}/'.format(sideband['dir'], preselection['dir'], sel['dir'])\n",
    "\n",
    "if not os.path.exists(SAVEPATH):\n",
    "    os.makedirs(SAVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVEFIG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ls)\n",
    "\n",
    "#choose what you want to show\n",
    "VARIABLE = 'reco_e'\n",
    "TITLE = r'reconstructed neutrino energy [GeV]'\n",
    "BINEDGES = np.linspace(0.15,1.55,15)\n",
    "\n",
    "print (BINEDGES)\n",
    "\n",
    "#output errors\n",
    "ERROR_OUT_V = []\n",
    "QUAD_ERROR_V = np.zeros(len(BINEDGES)-1)\n",
    "\n",
    "#fig = plt.figure(figsize=(20,3*len(DETVAR_N_V)))\n",
    "\n",
    "for idx,df_perm in enumerate(DETSYS_SAMPLE_V):\n",
    "    df = df_perm.copy()\n",
    "    print(\"starting {}...\".format(DETVAR_N_V[idx]))\n",
    "    \n",
    "    VARS_CV = _selection(VARIABLE+'_CV',df,QUERY_CV)\n",
    "\n",
    "    VARS_VAR = _selection(VARIABLE+'_VAR',df,QUERY_VAR)\n",
    "\n",
    "    #######################################\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    gs = fig.add_gridspec(1, 2)\n",
    "    \n",
    "    axis = fig.add_subplot(gs[0, 0])    \n",
    "    ################################\n",
    "    # CV-VAR histogram comparison\n",
    "    #get queried arrays of the variable\n",
    "    #get number of entries in each bin for each sample. and plot hists\n",
    "    n_cv, bins, p = axis.hist(VARS_CV ,bins=BINEDGES,histtype='step',\\\n",
    "                              lw=2,color='k',label='CV')\n",
    "    n_var, bins, p = axis.hist(VARS_VAR,bins=BINEDGES,histtype='step',\\\n",
    "                               lw=2,color='r',label='var : %s'%DETVAR_N_V[idx])\n",
    "\n",
    "    print ('n CV  : ',n_cv)\n",
    "    print ('n VAR : ',n_var)\n",
    "    \n",
    "    bc = 0.5*(bins[1:]+bins[:-1]) #bin centers\n",
    "\n",
    "    cov,frac_cov,corr = COVARIANCE(n_cv,n_var) #calculate various matrices\n",
    "    error = np.sqrt(np.diag(frac_cov)) #systematic error is this\n",
    "\n",
    "    if (DETVAR_N_V[idx] != \"Recomb\"):\n",
    "        ERROR_OUT_V.append(error)\n",
    "        QUAD_ERROR_V += np.diag(frac_cov)\n",
    "   \n",
    "    axis.set_xlabel(TITLE)\n",
    "    axis.set_ylabel('Num. Entries',fontsize=16)\n",
    "    #plt.ylim(0,plt.gca().get_ylim()[1]*1.5)\n",
    "    axis.legend(fontsize=15,loc=\"best\")\n",
    "    #plt.title(SAMPLE)\n",
    "\n",
    "    ########################################\n",
    "    axis = fig.add_subplot(gs[0, 1])\n",
    "    #####################################\n",
    "    # Fractional Covariance\n",
    "    #be consistent when comparing multiple plots\n",
    "    pos = axis.imshow(frac_cov, origin='lower', cmap='viridis',vmin=-0.025,vmax=0.025) \n",
    "    #print values onto the plot\n",
    "    # Limits for the extent\n",
    "    x_start = 0\n",
    "    x_end = len(n_cv)#-1\n",
    "    y_start = 0\n",
    "    y_end = len(n_cv)#-1\n",
    "    size = len(n_cv)#-1\n",
    "    jump_x = (x_end - x_start) / (2.0 * size)\n",
    "    jump_y = (y_end - y_start) / (2.0 * size)\n",
    "    x_positions = np.linspace(start=x_start, stop=x_end, num=size, endpoint=False)\n",
    "    y_positions = np.linspace(start=y_start, stop=y_end, num=size, endpoint=False)\n",
    "    for x_index, x in enumerate(x_positions):\n",
    "        #for x_index, x in enumerate(x_positions):\n",
    "        ERR = frac_cov[x_index, x_index]\n",
    "        label = \"{:.1f}\".format(100.*np.sqrt(ERR))\n",
    "        text_x = x #+ jump_x\n",
    "        text_y = x #+ jump_y\n",
    "        if (100.*np.sqrt(ERR) > 8):\n",
    "            axis.text(text_x, text_y, label, color='black', ha='center', va='center',fontsize=8)\n",
    "        else:\n",
    "            axis.text(text_x, text_y, label, color='white', ha='center', va='center',fontsize=8)\n",
    "    fig.colorbar(pos, ax=axis)\n",
    "    axis.set_ylabel(\"Bin number\")\n",
    "    axis.set_xlabel(\"Bin number\")\n",
    "    axis.set_title(sel_key)\n",
    "            \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if (SAVEFIG):\n",
    "        fig.savefig(SAVEPATH+VARIABLE+\"_%s.pdf\"%(DETVAR_N_V[idx]))\n",
    "        \n",
    "        # diagonal errors\n",
    "        fout = open(SAVEPATH+VARIABLE+\"_%s.txt\"%(DETVAR_N_V[idx]),'w')\n",
    "        for i,e in enumerate(error):\n",
    "            fout.write('%.02f - %.02f, %.03f \\n'%(BINEDGES[i],BINEDGES[i+1],e))\n",
    "        fout.close()\n",
    "        # covariance matrix\n",
    "        fout = open(SAVEPATH+VARIABLE+\"_%s_cov.txt\"%(DETVAR_N_V[idx]),'w')\n",
    "        string = ''\n",
    "        for n in range(len(error)):\n",
    "            string += '%.02f-%.02f, '%(BINEDGES[n],BINEDGES[n+1])\n",
    "        string += '\\n'\n",
    "        fout.write(string)\n",
    "        for i in range(len(error)):\n",
    "            string = '%.02f-%.02f, '%(BINEDGES[i],BINEDGES[i+1])\n",
    "            for j in range(len(error)):\n",
    "                string += '%.04f, '%frac_cov[i,j]\n",
    "            string += '\\n'\n",
    "            fout.write(string)\n",
    "        fout.close()\n",
    "            \n",
    "    \n",
    "SAVEFIG = False\n",
    "\n",
    "print (ERROR_OUT_V)\n",
    "print (np.array(np.sqrt(QUAD_ERROR_V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Plotter.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
