{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Systematic Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import localSettings as ls\n",
    "import os\n",
    "print(ls.main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1560556807118,
     "user": {
      "displayName": "Stefano Roberto Soleti",
      "photoUrl": "https://lh4.googleusercontent.com/-hfLpspJu4Q0/AAAAAAAAAAI/AAAAAAAABmA/2kE4rtj8paU/s64/photo.jpg",
      "userId": "10372352518008961760"
     },
     "user_tz": 240
    },
    "id": "6qsD0G-yYJ9K",
    "outputId": "5d52a3ec-50be-44fc-da44-3c0593e98bc6"
   },
   "outputs": [],
   "source": [
    "main_path = ls.main_path\n",
    "sys.path.append(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m%d%Y\")\n",
    "print(\"date and time:\",date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Which type(s?) of study\n",
    "#########################\n",
    "#which study?\n",
    "NUE = False\n",
    "NUMU = True\n",
    "PI0 = False\n",
    "CCmuCPiNoPi0 = False\n",
    "CCmuNoPi = False\n",
    "NCNoPi = False\n",
    "NCcPiNoPi0 = False\n",
    "\n",
    "\n",
    "#make calculated columns?\n",
    "NUEVARIABLES = False\n",
    "NUMUVARIABLES = True\n",
    "\n",
    "#other globals for notebook\n",
    "SAVEFIG = False\n",
    "USECRT = True\n",
    "USEBDT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGqTJ5JgaDsx"
   },
   "outputs": [],
   "source": [
    "import plotter\n",
    "import importlib\n",
    "importlib.reload(plotter)\n",
    "import uproot\n",
    "import matplotlib.pylab as pylab\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "if USEBDT:\n",
    "    import xgboost as xgb\n",
    "    import nue_booster \n",
    "    importlib.reload(nue_booster)\n",
    "import awkward\n",
    "import pandas as pd\n",
    "\n",
    "params = {\n",
    "    'axes.labelsize': 'x-large',\n",
    "    'axes.titlesize': 'x-large',\n",
    "    'xtick.labelsize': 'x-large',\n",
    "    'ytick.labelsize': 'x-large'\n",
    "}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = r'$\\nu_e$'\n",
    "SAMPLEDEF = 'nue'\n",
    "if (PI0):\n",
    "    SAMPLE = r\"$\\pi^0$\"\n",
    "    SAMPLEDEF = 'pi0'\n",
    "if (NUMU):\n",
    "    SAMPLE = r'$\\nu_{\\mu}$'\n",
    "    SAMPLEDEF = 'numu'\n",
    "if (CCmuCPiNoPi0):\n",
    "    SAMPLE = r'$\\nu_{\\mu}$ CC $\\pi$ no $\\pi^0$'\n",
    "    SAMPLEDEF = 'CCmuCPiNoPi0'\n",
    "if (CCmuNoPi):\n",
    "    SAMPLE = r'$\\nu_{\\mu}$ CC no $\\pi$'\n",
    "    SAMPLEDEF = 'CCmuNoPi'\n",
    "if (NCNoPi):\n",
    "    SAMPLE = r'$\\nu_{\\mu}$ NC no $\\pi$'\n",
    "    SAMPLEDEF = 'NCNoPi'\n",
    "if (NCcPiNoPi0):\n",
    "    SAMPLE = r'$\\nu_{\\mu}$ NC $\\pi$ no $\\pi^0$'\n",
    "    SAMPLEDEF = 'NCcPiNoPi0'\n",
    "SAMPLE += ' sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    \"selected\", \"nu_pdg\", \n",
    "    \"slpdg\", \"trk_score_v\", \"backtracked_pdg\", # modified from shr_score_v\n",
    "    \"category\",\n",
    "    \"topological_score\",\n",
    "    \"trk_energy_tot\", \"shr_hits_tot\", \"ccnc\", \"trk_chipr\",\n",
    "    \"trk_bkt_pdg\", \"trk_bkt_purity\",\"hits_ratio\", \"n_tracks_contained\", \n",
    "    \"crtveto\",\"crthitpe\",\"_closestNuCosmicDist\",\n",
    "    \"NeutrinoEnergy2\",\n",
    "    \"ccnc\",\"flash_pe\",\n",
    "    #\"run\",\"sub\",\"evt\",\n",
    "    \"CosmicIP\",\"CosmicDirAll3D\",\"CosmicIPAll3D\",\n",
    "    \"nu_flashmatch_score\",\"best_cosmic_flashmatch_score\",\"best_obviouscosmic_flashmatch_score\",\n",
    "    #\"trk_pfp_id\",\n",
    "    \"true_nu_vtx_x\",\"true_nu_vtx_y\",\"true_nu_vtx_z\",\n",
    "    \"reco_nu_vtx_x\", \"reco_nu_vtx_y\", \"reco_nu_vtx_z\",\n",
    "    \"reco_nu_vtx_sce_x\",\"reco_nu_vtx_sce_y\",\"reco_nu_vtx_sce_z\",\n",
    "    \"shrsubclusters0\",\"shrsubclusters1\",\"shrsubclusters2\", # number of sub-clusters in shower\n",
    "    \"trk_llr_pid_score_v\", # trk-PID score\n",
    "    \"trk_energy_proton_v\", # track energy under proton hyp\n",
    "    \"trk_calo_energy_y_v\", # track calo energy\n",
    "    \"_opfilter_pe_beam\", \"_opfilter_pe_veto\", # did the event pass the common optical filter (for MC only)\n",
    "    \"nproton\", \"nu_e\", \"n_showers_contained\", \"shr_distance\", \"trk_distance\",\"proton_e\",\"elec_e\",\n",
    "    \"hits_y\", \"shr_pz\", \"shr_energy\", \"shr_dedx_U\", \"shr_dedx_V\", \"shr_phi\", \"trk_phi\", \"trk_theta\",\n",
    "    \"run\", \"sub\", \"evt\", \"trk_pid_chipr_v\",\n",
    "    \"trk_len\", \"mc_pdg\", \"slnunhits\", \"slnhits\", \"shr_score\", \"trk_score\", \"trk_hits_tot\",\n",
    "    \"true_e_visible\", \"matched_E\", \"shr_bkt_E\", \"trk_bkt_E\", \"trk_energy\", \"tksh_distance\", \"tksh_angle\",\n",
    "    \"pi0_e\", \"shr_energy_tot_cali\", \"shr_dedx_Y_cali\", \"evnunhits\", \"nslice\", \"interaction\",\n",
    "    \"slclustfrac\", \"contained_fraction\",\n",
    "    \n",
    "    \"weightSpline\",\"weightSplineTimesTune\",\"weightTune\",\n",
    "    \n",
    "    # truth variables\n",
    "    \"isVtxInFiducial\",\"truthFiducial\",\n",
    "]\n",
    "\n",
    "if NUE or NUEVARIABLES:\n",
    "    variables += [\n",
    "        \"shr_dedx_Y\", \"shr_bkt_pdg\",\"shr_bkt_purity\", \"p\", \"pt\", \"shr_theta\",\n",
    "        \"shr_pfp_id_v\",\n",
    "        \"shr_tkfit_dedx_U\",\"shr_tkfit_dedx_V\",\"shr_tkfit_dedx_Y\",\n",
    "        \"shr_tkfit_gap10_dedx_U\",\"shr_tkfit_gap10_dedx_V\",\"shr_tkfit_gap10_dedx_Y\",\n",
    "        \"shr_tkfit_2cm_dedx_U\",\"shr_tkfit_2cm_dedx_V\",\"shr_tkfit_2cm_dedx_Y\",#\"shr_energy_tot\", \n",
    "        \"shrmoliereavg\",\"shrmoliererms\",\n",
    "        \"shr_tkfit_npointsvalid\",\"shr_tkfit_npoints\", # fitted vs. all hits for shower\n",
    "        \"shrclusfrac0\",\"shrclusfrac1\",\"shrclusfrac2\", # track-fitted hits / all hits\n",
    "        \"shr_tkfit_dedx_U\", \"shr_tkfit_dedx_V\",\n",
    "        \"secondshower_Y_nhit\",\"secondshower_Y_vtxdist\",\"secondshower_Y_dot\",\"secondshower_Y_dir\",\"shrclusdir2\",\n",
    "        \"shr_tkfit_nhits_Y\",\"shr_tkfit_nhits_U\",\"shr_tkfit_nhits_V\",\n",
    "        \"shr_tkfit_2cm_nhits_Y\",\"shr_tkfit_2cm_nhits_U\",\"shr_tkfit_2cm_nhits_V\",\n",
    "        \"shr_tkfit_gap10_nhits_Y\",\"shr_tkfit_gap10_nhits_U\",\"shr_tkfit_gap10_nhits_V\",\n",
    "        \"trkshrhitdist2\", # \"trkshrhitdist0\",\"trkshrhitdist1\", distance between track and shower in 2D\n",
    "    ]\n",
    "if NUMU or NUMUVARIABLES:\n",
    "    variables += [\n",
    "        'trk_mcs_muon_mom_v','trk_range_muon_mom_v','trk_energy_muon',\n",
    "        'trk_sce_start_x_v','trk_sce_start_y_v','trk_sce_start_z_v',\n",
    "        'trk_sce_end_x_v','trk_sce_end_y_v','trk_sce_end_z_v',\n",
    "        'trk_theta_v', 'trk_len_v',\n",
    "        'pfp_generation_v', 'trk_distance_v'\n",
    "    ]\n",
    "    \n",
    "if PI0 or CCmuCPiNoPi0 or CCmuNoPi or NCcPiNoPi0:\n",
    "    variables += [\n",
    "        # pi0 variables\n",
    "        \"pi0_radlen1\",\"pi0_radlen2\",\"pi0_dot1\",\"pi0_dot2\",\"pi0_energy1_Y\",\"pi0_energy2_Y\",\n",
    "        \"pi0_dedx1_fit_Y\",\"pi0_dedx2_fit_Y\",\"pi0_shrscore1\",\"pi0_shrscore2\",\"pi0_gammadot\",\n",
    "        \"pi0_dedx1_fit_V\",\"pi0_dedx2_fit_V\",\"pi0_dedx1_fit_U\",\"pi0_dedx2_fit_U\",\n",
    "        \"pi0_mass_Y\",\"pi0_mass_V\",\"pi0_mass_U\",\n",
    "        \"pi0_dir2_x\",\"pi0_dir2_y\",\"pi0_dir2_z\",\"pi0_dir1_x\",\"pi0_dir1_y\",\"pi0_dir1_z\",\n",
    "        #\"pi0_energy2_Y\", # pi0 tagger variables\n",
    "        \"npi0\",\"npion\",\"pion_e\",\"muon_e\",\"pi0truth_elec_etot\",\n",
    "    ]\n",
    "\n",
    "# variables to be trained on\n",
    "TRAINVAR = [\"shr_score\",\"tksh_distance\",\"tksh_angle\",\n",
    "            \"shr_tkfit_dedx_max\",\n",
    "            \"trkfit\",\"trkpid\",\n",
    "            \"subcluster\",\"shrmoliereavg\",\n",
    "            \"trkshrhitdist2\",\"hits_ratio\",\n",
    "            \"secondshower_Y_nhit\",\"secondshower_Y_vtxdist\",\"secondshower_Y_dot\",\"anglediff_Y\",\n",
    "            \"CosmicIPAll3D\",\"CosmicDirAll3D\"]\n",
    "\n",
    "BDT_LABELS =  ['pi0','nonpi0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pi0 Selection\n",
    "\n",
    "# pi0 selection\n",
    "LOOSE = False\n",
    "if (LOOSE):\n",
    "    SCORECUT = 0.8 # 0.75 #75 # max track score\n",
    "    DVTX = 3.0 # 3. # distance from vertex of each shower\n",
    "    VTXDOT = 0.8 # dot product between each shower's direction and the vtx -> shr start vector\n",
    "    EMIN1 =  50. # leading photon min energy\n",
    "    EMIN2 =  20. #20. # 20. # subleading photon min energy\n",
    "    GAMMADOT = 0.94 # max dot product between showres\n",
    "    DEDXCUT = 0.0 # MeV/cm cut on leading shower only\n",
    "else:\n",
    "    SCORECUT = 0.5 # 0.75 #75 # max track score\n",
    "    DVTX = 3.0 # 3. # distance from vertex of each shower\n",
    "    VTXDOT = 0.8 # dot product between each shower's direction and the vtx -> shr start vector\n",
    "    EMIN1 =  60. # leading photon min energy\n",
    "    EMIN2 =  40. #20. # 20. # subleading photon min energy\n",
    "    GAMMADOT = 0.94 # max dot product between showres\n",
    "    DEDXCUT = 1.0 # MeV/cm cut on leading shower only\n",
    "\n",
    "CUT_VAR_V = [\"nslice\",\"pi0_shrscore1\",\"pi0_shrscore2\",\"pi0_dot1\",\"pi0_dot2\",\\\n",
    "            \"pi0_radlen1\",\"pi0_radlen2\",\"pi0_gammadot\",\"pi0_energy1_Y\",\"pi0_energy2_Y\",\\\n",
    "            \"pi0_dedx1_fit_Y\",\"n_showers_contained\"]\n",
    "CUT_VAL_V = [\" == 1\",\" < %f\"%SCORECUT,\" < %f\"%SCORECUT,\" > %f\"%VTXDOT,\" > %f\"%VTXDOT,\\\n",
    "            \" > %f\"%DVTX,\" > %f\"%DVTX,\" < %f\"%GAMMADOT,\" > %f\"%EMIN1,\" > %f\"%EMIN2,\\\n",
    "            \">= %f\"%DEDXCUT,\" != 0\"]\n",
    "\n",
    "def Pi0Query(APP):\n",
    "\n",
    "    QUERY = \"\"\n",
    "    \n",
    "    for i,v in enumerate(CUT_VAR_V):\n",
    "        if (i == 0):\n",
    "            QUERY  += '%s_%s %s'%(v,APP,CUT_VAL_V[i])\n",
    "        else:\n",
    "            QUERY  += ' and %s_%s %s'%(v,APP,CUT_VAL_V[i])   \n",
    "            \n",
    "    return QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NpBoxCutQuery(APP,BDT=False):    \n",
    "    QUERY = ''   \n",
    "    # nue preselection\n",
    "    PRESQ = 'nslice_%s == 1'%APP\n",
    "    PRESQ += ' and selected_%s == 1'%APP\n",
    "    PRESQ += ' and shr_energy_tot_cali_%s > 0.07'%APP\n",
    "    PRESQ += ' and _opfilter_pe_beam_%s > 0 and _opfilter_pe_veto_%s < 20'%(APP,APP)\n",
    "    \n",
    "    #return PRESQ\n",
    "    \n",
    "    # 1eNp preselection\n",
    "    NPPRESQ = PRESQ\n",
    "    NPPRESQ += ' and n_tracks_contained_%s > 0'%APP\n",
    "    \n",
    "    return NPPRESQ\n",
    "    \n",
    "    # loose box cuts\n",
    "    NPLCUTQ = NPPRESQ\n",
    "    NPLCUTQ += ' and CosmicIPAll3D_%s > 10.'%APP\n",
    "    #NPLCUTQ += ' and trkpid_%s < 0.02'%APP\n",
    "    NPLCUTQ += ' and hits_ratio_%s > 0.50'%APP\n",
    "    NPLCUTQ += ' and shrmoliereavg_%s < 9'%APP\n",
    "    NPLCUTQ += ' and subcluster_%s > 4'%APP\n",
    "    NPLCUTQ += ' and trkfit_%s < 0.65'%APP\n",
    "    NPLCUTQ += ' and n_showers_contained_%s == 1'%APP\n",
    "    NPLCUTQ += ' and tksh_distance_%s < 6.0'%APP\n",
    "    NPLCUTQ += ' and (shr_tkfit_nhits_tot_%s > 1 and shr_tkfit_dedx_max_%s > 0.5 and shr_tkfit_dedx_max_%s < 5.5)'%(APP,APP,APP)\n",
    "    NPLCUTQ += ' and secondshower_Y_nhit_%s < 50'%APP\n",
    "    NPLCUTQ += ' and tksh_angle_%s > -0.9'%APP\n",
    "    \n",
    "    return NPLCUTQ\n",
    "    \n",
    "    # tight box cuts\n",
    "    NPTCUTQ = NPLCUTQ\n",
    "    NPTCUTQ += ' and CosmicIPAll3D_%s > 30.'%APP\n",
    "    NPTCUTQ += ' and CosmicDirAll3D_%s > -0.98 and CosmicDirAll3D_%s < 0.98'%(APP,APP)\n",
    "    #NPTCUTQ += ' and trkpid_%s < 0.02'%APP\n",
    "    NPTCUTQ += ' and hits_ratio_%s > 0.65'%APP\n",
    "    NPTCUTQ += ' and shr_score_%s < 0.25'%APP\n",
    "    NPTCUTQ += ' and shrmoliereavg_%s > 2 and shrmoliereavg_%s < 10'%(APP,APP)\n",
    "    NPTCUTQ += ' and subcluster_%s > 7'%APP\n",
    "    NPTCUTQ += ' and trkfit_%s < 0.70'%APP\n",
    "    NPTCUTQ += ' and n_showers_contained_%s == 1'%APP\n",
    "    NPTCUTQ += ' and tksh_distance_%s < 4.0'%APP\n",
    "    NPTCUTQ += ' and trkshrhitdist2_%s < 1.5'%APP\n",
    "    NPTCUTQ += ' and (shr_tkfit_nhits_tot_%s > 1 and shr_tkfit_dedx_max_%s > 1.0 and shr_tkfit_dedx_max_%s < 3.8)'%(APP,APP,APP)\n",
    "    NPTCUTQ += ' and (secondshower_Y_nhit_%s <= 8 or secondshower_Y_dot_%s <= 0.8 or anglediff_Y_%s <= 40 or secondshower_Y_vtxdist_%s >= 100)'%(APP,APP,APP,APP)\n",
    "    NPTCUTQ += ' and secondshower_Y_nhit_%s < 30'%APP\n",
    "    NPTCUTQ += ' and tksh_angle_%s > -0.9 and tksh_angle_%s < 0.70'%(APP,APP)\n",
    "    \n",
    "    # BDT cuts\n",
    "    # 0304 extnumi, pi0 and nonpi0\n",
    "    if (BDT):\n",
    "        BDTCQ = NPLCUTQ\n",
    "        BDTCQ += ' and pi0_score_%s > 0.67 and nonpi0_score_%s > 0.70'%(APP,APP)\n",
    "        return BDTCQ\n",
    "    \n",
    "    return NPTCUTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NUMU Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Cuts for NUMU constraint selection\n",
    "####################################\n",
    "# returns QUERY, track_cuts for given appendix ('CV' OR 'VAR')\n",
    "#updated for SCE\n",
    "#will swap numerical vals at end\n",
    "FVx = [10,246]#[5,251]\n",
    "FVy = [-110,110]\n",
    "FVz = [20,986]\n",
    "\n",
    "def NUMUQuery(APPEND,verbose=False):\n",
    "    query = 'nslice_{} == 1'.format(APPEND)\n",
    "    query += ' and topological_score_{} > 0.06'.format(APPEND)\n",
    "    query += ' and reco_nu_vtx_sce_x_{} > FVx[0] and reco_nu_vtx_sce_x_{} < FVx[1]'.format(APPEND,APPEND)\n",
    "    query += ' and reco_nu_vtx_sce_y_{} > FVy[0] and reco_nu_vtx_sce_y_{} < FVy[1]'.format(APPEND,APPEND)\n",
    "    query += ' and reco_nu_vtx_sce_z_{} > FVz[0] and reco_nu_vtx_sce_z_{} < FVz[1]'.format(APPEND,APPEND)\n",
    "    query += ' and ( (reco_nu_vtx_sce_z_{} < 675) or (reco_nu_vtx_sce_z_{} > 775) )'.format(APPEND,APPEND) #avoid dead wire region    \n",
    "    if USECRT: query += ' and (crtveto_{}!=1 or crthitpe_{} < 100.) and (_closestNuCosmicDist_{} > 20.)'.format(APPEND,APPEND,APPEND)\n",
    "\n",
    "    query = query.replace('FVx[0]',str(FVx[0]))\n",
    "    query = query.replace('FVy[0]',str(FVy[0]))\n",
    "    query = query.replace('FVz[0]',str(FVz[0]))\n",
    "    query = query.replace('FVx[1]',str(FVx[1]))\n",
    "    query = query.replace('FVy[1]',str(FVy[1]))\n",
    "    query = query.replace('FVz[1]',str(FVz[1])) \n",
    "\n",
    "    if verbose: print (\"QUERY: \\n\",query)\n",
    "\n",
    "    track_cuts = [\n",
    "        ('trk_sce_start_x_v_{}'.format(APPEND), '>', FVx[0]),\n",
    "        ('trk_sce_start_x_v_{}'.format(APPEND), '<', FVx[1]),\n",
    "        ('trk_sce_start_y_v_{}'.format(APPEND), '>', FVy[0]),\n",
    "        ('trk_sce_start_y_v_{}'.format(APPEND), '<', FVy[1]),\n",
    "        ('trk_sce_start_z_v_{}'.format(APPEND), '>', FVz[0]),\n",
    "        ('trk_sce_start_z_v_{}'.format(APPEND), '<', FVz[1]),\n",
    "        ('trk_sce_end_x_v_{}'.format(APPEND), '>', FVx[0]),\n",
    "        ('trk_sce_end_x_v_{}'.format(APPEND), '<', FVx[1]),\n",
    "        ('trk_sce_end_y_v_{}'.format(APPEND), '>', FVy[0]),\n",
    "        ('trk_sce_end_y_v_{}'.format(APPEND), '<', FVy[1]),\n",
    "        ('trk_sce_end_z_v_{}'.format(APPEND), '>', FVz[0]),\n",
    "        ('trk_sce_end_z_v_{}'.format(APPEND), '<', FVz[1]),\n",
    "        ('trk_p_quality_v_{}'.format(APPEND), '>', -0.5),\n",
    "        ('trk_p_quality_v_{}'.format(APPEND), '<', 0.5),\n",
    "        ('trk_llr_pid_score_v_{}'.format(APPEND), '>', 0.2),\n",
    "        ('trk_score_v_{}'.format(APPEND), '>', 0.8),\n",
    "        ('trk_len_v_{}'.format(APPEND), '>', 10),\n",
    "        ('pfp_generation_v_{}'.format(APPEND), '==', 2),\n",
    "        ('trk_distance_v_{}'.format(APPEND), '<', 4)\n",
    "    ]\n",
    "\n",
    "    if verbose: \n",
    "        print(\"\\ntrack cuts:\")\n",
    "        cut_string = \"\"\n",
    "        for c,cut in enumerate(track_cuts):\n",
    "            if c > 0: cut_string += \" and \"\n",
    "            if type(cut[1]) == list: cut_string += \"( ({} {} {}) or ({} {} {}) )\".format(cut[0],cut[1][0],cut[2][0],cut[0],cut[1][1],cut[2][1])\n",
    "            else: cut_string += \"{} {} {}\".format(cut[0],cut[1],cut[2])\n",
    "        print(cut_string)\n",
    "    \n",
    "    return query, track_cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrames and make Calculated Columns\n",
    "#### Uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ls)\n",
    "##################################################\n",
    "# Declare which dataframes to load and load uproot file\n",
    "# This is dependent on the state variables above\n",
    "##################################################\n",
    "fold = \"nuselection\"\n",
    "tree = \"NeutrinoSelectionFilter\"\n",
    "\n",
    "if (NUE):\n",
    "    # for TPC variations\n",
    "    NUECV = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_CV_reco2_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    \n",
    "    NUEWX = \"prodgenie_bnb_nue_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUEWYZ = \"prodgenie_bnb_nue_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUESCE = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_SCE_reco2_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUEWAYZ = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_WireModAngleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUEWAXZ = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_WireModAngleXZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUER2   = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_Recomb2_reco2_v08_00_00_39_run3b_reco2_reco2.root\"\n",
    "    NUEdEdx = \"prodgenie_bnb_nue_overlay_DetVar_wiremod_ScaledEdX_v08_00_00_39_run3b_reco2_reco2.root\"\n",
    "    \n",
    "    DETVAR_N_V = [\"WireMod X\", \"WireMod YZ\", 'SCE','WireMod angle YZ', \"WireMod Angle XZ\",\"Recomb\",\"WireMod dEdx\"]\n",
    "    DETVAR_S_V = [NUEWX,NUEWYZ,NUESCE,NUEWAYZ,NUEWAXZ,NUER2,NUEdEdx]\n",
    "    \n",
    "    # for LY variations\n",
    "    #NUECV = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_CV_reco2_v08_00_00_38_run1_reco2_reco2.root\"\n",
    "    #NUELY = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_LYDown_v08_00_00_37_run1_reco2_reco2.root\"\n",
    "    #NUERY = \"prodgenie_bnb_intrinsic_nue_overlay_DetVar_LYRayleigh_v08_00_00_37_run1_reco2_reco2.root\"\n",
    "    \n",
    "    #DETVAR_N_V = [\"LY Down\", \"Rayleigh\"]\n",
    "    #DETVAR_S_V = [NUELY,NUERY]\n",
    "    \n",
    "    CV = uproot.open(ls.ntuple_path+NUECV)[fold][tree]\n",
    "    #CVPOTSUB   = uproot.open(ls.main_path+NUECV)[fold][\"SubRun\"]\n",
    "    \n",
    "\n",
    "if (PI0):\n",
    "    \n",
    "    PI0CV  = \"prodgenie_cc_pi0_overlay_DetVar_CV_reco2_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    PI0WX  = \"prodgenie_cc_pi0_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    PI0WYZ = \"prodgenie_cc_pi0_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    PI0AXZ = \"prodgenie_data_cc_pi0_overlay_DetVar_WireModAngleXZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    PI0AYZ = \"prodgenie_data_cc_pi0_overlay_DetVar_WireModAngleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    \n",
    "    DETVAR_N_V = [\"WireMod X\",\"WireMod YZ\",\"WireMod angle XZ\",\"WireMod angle YZ\"]\n",
    "    DETVAR_S_V = [PI0WX,PI0WYZ,PI0AXZ,PI0AYZ]\n",
    "    \n",
    "    CV = uproot.open(ls.ntuple_path+PI0CV)[fold][tree]\n",
    "    \n",
    "if (NUMU):\n",
    "    NUMUCV = \"prodgenie_bnb_nu_overlay_DetVar_CV_reco2_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    \n",
    "    NUMULYATT = \"prodgenie_bnb_nu_overlay_DetVar_LYAttenuation_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUMULYDOWN = \"prodgenie_bnb_nu_overlay_DetVar_LYDown_v08_00_00_37_v2_run3b_reco2_reco2.root\"\n",
    "    NUMURY = \"prodgenie_bnb_nu_overlay_DetVar_LYRayleigh_v08_00_00_37_run3b_reco2_reco2.root\"\n",
    "    NUMUSCE = \"prodgenie_bnb_nu_overlay_DetVar_SCE_reco2_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUMUWX = \"prodgenie_bnb_nu_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUMUWYZ = \"prodgenie_bnb_nu_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    NUMUWAXZ = \"prodgenie_bnb_nu_overlay_DetVar_WireModAngleXZ_v08_00_00_38_exe_run3b_reco2_reco2.root\"\n",
    "    NUMUWAYZ = \"prodgenie_bnb_nu_overlay_DetVar_WireModAngleYZ_v08_00_00_38_exe_run3b_reco2_reco2.root\"\n",
    "    NUMUWDEDX = \"prodgenie_bnb_nu_overlay_DetVar_wiremod_ScaledEdX_v08_00_00_39_run3b_reco2_reco2.root\"\n",
    "    NUMURECOMB = \"prodgenie_bnb_nu_overlay_DetVar_Recomb2_reco2_v08_00_00_39_run3b_reco2_reco2.root\"\n",
    "\n",
    "    DETVAR_N_V = ['LY Attenuation','LY Down','Rayleigh','SCE',\n",
    "                  'WireMod X','WireMod YZ',\n",
    "                  'WireMod Angle XZ','WireMod Angle YZ',\n",
    "                 'WireMod dEdX', 'Recomb']\n",
    "\n",
    "    DETVAR_S_V = [NUMULYATT,NUMULYDOWN, NUMURY, NUMUSCE,\n",
    "                  NUMUWX, NUMUWYZ,\n",
    "                  NUMUWAXZ, NUMUWAYZ,\n",
    "                 NUMUWDEDX,NUMURECOMB]\n",
    "    \n",
    "    CV = uproot.open(ls.ntuple_path+NUMUCV)[fold][tree]\n",
    "    \n",
    "if (CCmuCPiNoPi0):\n",
    "    \n",
    "    CVF = \"prodgenie_filter_CCmuCPiNoPi0_overlay_mcc9_v08_00_00_33_run3_reco2_reco2.root\"\n",
    "    CV = uproot.open(ls.ntuple_path+CVF)[fold][tree]\n",
    "    \n",
    "    VX = \"prodgenie_CCmuCPiNoPi0_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    VYZ = \"prodgenie_CCmuCPiNoPi0_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    \n",
    "    DETVAR_N_V = [\"WireMod X\",\"WireMod YZ\"]\n",
    "    DETVAR_S_V = [VX,VYZ]\n",
    "    \n",
    "if (CCmuNoPi):\n",
    "    \n",
    "    CVF = \"prodgenie_tight_filter_CCmuNoPi_mcc9_v08_00_00_35_run3_reco2_reco2.root\"\n",
    "    CV = uproot.open(ls.ntuple_path+CVF)[fold][tree]\n",
    "    \n",
    "    VX = \"prodgenie_CCmuNoPi_tight_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    VYZ = \"prodgenie_CCmuNoPi_tight_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "\n",
    "    DETVAR_N_V = [\"WireMod X\",\"WireMod YZ\"]\n",
    "    DETVAR_S_V = [VX,VYZ]\n",
    "    \n",
    "if (NCNoPi):\n",
    "    \n",
    "    CVF = \"prodgenie_tight_filter_NCNoPi_mcc9_v08_00_00_35_run3_reco2_reco2.root\"\n",
    "    CV = uproot.open(ls.ntuple_path+CVF)[fold][tree]\n",
    "    \n",
    "    VX = \"prodgenie_NCNoPi_tight_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    VYZ = \"prodgenie_NCNoPi_tight_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "\n",
    "    DETVAR_N_V = [\"WireMod X\",\"WireMod YZ\"]\n",
    "    DETVAR_S_V = [VX,VYZ]\n",
    "        \n",
    "if (NCcPiNoPi0):\n",
    "    \n",
    "    CVF = \"prodgenie_NCcPiNoPi0_overlay_mcc9_v08_00_00_33_New_run3_reco2_reco2.root\"\n",
    "    CV = uproot.open(ls.ntuple_path+CVF)[fold][tree]\n",
    "    \n",
    "    VX = \"prodgenie_NCcPiNoPi0_overlay_DetVar_wiremod_ScaleX_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "    VYZ = \"prodgenie_NCcPiNoPi0_overlay_DetVar_wiremod_ScaleYZ_v08_00_00_38_run3b_reco2_reco2.root\"\n",
    "\n",
    "    DETVAR_N_V = [\"WireMod X\",\"WireMod YZ\"]\n",
    "    DETVAR_S_V = [VX,VYZ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ls)\n",
    "    \n",
    "###############################\n",
    "# Central Value DF\n",
    "CVDF  = CV.pandas.df(variables, flatten=False)\n",
    "\n",
    "#CVPOTDF = CV.pandas.df(['run','sub'],flatten=False)\n",
    "#CVPOTSUBDF = CVPOTSUB.pandas.df(['run','subRun','pot'],flatten=False)\n",
    "#CVPOTSUBDF = CVPOTSUBDF.rename(columns={\"subRun\": \"sub\",\"run\":\"run\"})\n",
    "\n",
    "CVDF['identifier'] = CVDF['run']*100000 + CVDF['evt'] #to line up events with sample events\n",
    "\n",
    "CVDF.loc[ CVDF['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "CVDF.loc[ CVDF['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "CVDF.loc[ CVDF['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "CVDF.loc[ np.isnan(CVDF['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "\n",
    "# if running selections, define all necessary variables\n",
    "if (NUEVARIABLES):\n",
    "    INTERCEPT = 0.0\n",
    "    SLOPE = 0.83\n",
    "    trk_llr_pid_v = CV.array('trk_llr_pid_score_v')\n",
    "    trk_energy_proton_v = CV.array('trk_energy_proton_v')\n",
    "    trk_calo_energy_y_v = CV.array('trk_calo_energy_y_v')\n",
    "    trk_id = CV.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "    trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "    trk_energy_proton_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_energy_proton_v,trk_id)])\n",
    "    trk_calo_energy_y_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_calo_energy_y_v,trk_id)])\n",
    "    CVDF['trkpid'] = trk_llr_pid_v_sel\n",
    "    CVDF['protonenergy'] = trk_energy_proton_sel\n",
    "    CVDF['trackcaloenergy'] = trk_calo_energy_y_sel\n",
    "    CVDF['subcluster'] = CVDF['shrsubclusters0'] + CVDF['shrsubclusters1'] + CVDF['shrsubclusters2']\n",
    "    CVDF['trkfit'] = CVDF['shr_tkfit_npointsvalid'] / CVDF['shr_tkfit_npoints']\n",
    "    CVDF['anglediff_Y'] = np.abs(CVDF['secondshower_Y_dir']-CVDF['shrclusdir2'])\n",
    "    CVDF['shr_tkfit_nhits_tot'] = (CVDF['shr_tkfit_nhits_Y']+CVDF['shr_tkfit_nhits_U']+CVDF['shr_tkfit_nhits_V'])\n",
    "    CVDF.loc[:,'shr_tkfit_dedx_max'] = CVDF['shr_tkfit_dedx_Y']\n",
    "    CVDF.loc[(CVDF['shr_tkfit_nhits_U']>CVDF['shr_tkfit_nhits_Y']),'shr_tkfit_dedx_max'] = CVDF['shr_tkfit_dedx_U']\n",
    "    CVDF.loc[(CVDF['shr_tkfit_nhits_V']>CVDF['shr_tkfit_nhits_Y']) & (CVDF['shr_tkfit_nhits_V']>CVDF['shr_tkfit_nhits_U']),'shr_tkfit_dedx_max'] = CVDF['shr_tkfit_dedx_V']\n",
    "    CVDF[\"reco_e\"] = (CVDF[\"shr_energy_tot_cali\"] + INTERCEPT) / SLOPE + CVDF[\"trk_energy_tot\"] \n",
    "    CVDF[\"is_signal\"] = CVDF[\"category\"] == 11\n",
    "\n",
    "    if (USEBDT == True):\n",
    "        for label, bkg_query in zip(BDT_LABELS, nue_booster.bkg_queries):\n",
    "            with open(ls.pickle_path+'booster_%s_0304_extnumi.pickle' % label, 'rb') as booster_file:\n",
    "                booster = pickle.load(booster_file)\n",
    "                CVDF[label+\"_score\"] = booster.predict(\n",
    "                    xgb.DMatrix(CVDF[TRAINVAR]),\n",
    "                    ntree_limit=booster.best_iteration)\n",
    "\n",
    "if (NUMUVARIABLES):\n",
    "    M_mu = 0.105 #GeV/c\n",
    "    CVDF['trk_p_quality_v'] = (CVDF['trk_mcs_muon_mom_v']-CVDF['trk_range_muon_mom_v'])/CVDF['trk_range_muon_mom_v']\n",
    "    CVDF['trk_costheta_v'] = CVDF['trk_theta_v'].apply(lambda x: np.cos(x))\n",
    "    CVDF['reco_nu_e_range'] = CVDF[\"trk_energy_muon\"] + (CVDF[\"trk_energy_tot\"] - CVDF[\"trk_energy\"]) + M_mu\n",
    "    CVDF['reco_muon_e_range'] = CVDF[\"trk_energy_muon\"] + M_mu\n",
    "\n",
    "print ('there are %i CV events'%(CVDF.shape[0]))\n",
    "\n",
    "DETSYS_SAMPLE_V = [] #list of merged CV-VAR dfataframes\n",
    "POT_V = [] #POT of samples? Not really used elsewhere\n",
    "\n",
    "for i,N in enumerate(DETVAR_N_V):\n",
    "    \n",
    "    VAR = uproot.open(ls.ntuple_path+DETVAR_S_V[i])[fold][tree]\n",
    "    VARCVDF = VAR.pandas.df(variables, flatten=False)\n",
    "    \n",
    "    #VARPOTDF = VAR.pandas.df(['run','sub'],flatten=False)\n",
    "    \n",
    "    # intersect VARPOTDF and CVPOTDF to get appropriate POT\n",
    "    #INTPOT = pd.merge(CVPOTDF,VARPOTDF,how='inner',on=['run','sub'])\n",
    "    #print (INTPOT)\n",
    "    \n",
    "    #VARPOTSUB   = uproot.open(ls.ntuple_path+DETVAR_S_V[i])[fold][\"SubRun\"]\n",
    "    #VARPOTSUBDF = VARPOTSUB.pandas.df(['run','subRun','pot'],flatten=False)\n",
    "    #VARPOTSUBDF = VARPOTSUBDF.rename(columns={\"subRun\": \"sub\",\"run\":\"run\"})\n",
    "    \n",
    "    #POTSUM = np.sum(VARPOTSUBCVDF['pot'].values)\n",
    "    #print ('POT sum before merge is ',POTSUM)\n",
    "    \n",
    "    #INTPOTSUBDF = pd.merge(CVPOTSUBDF,VARPOTSUBDF,how='inner',on=['run','sub'],suffixes=('_DF','_POT'))\n",
    "    #print (INTPOTSUBDF)\n",
    "    \n",
    "    #POTSUM = np.sum(INTPOTSUBCVDF['pot_POT'].values)/2.\n",
    "    #print ('POT sum after merge is ',POTSUM)\n",
    "    \n",
    "    POT_V.append(6e22)\n",
    "    \n",
    "    VARCVDF['identifier'] = VARCVDF['run']*100000 + VARCVDF['evt']\n",
    "    \n",
    "    VARCVDF.loc[ VARCVDF['weightSplineTimesTune'] <= 0, 'weightSplineTimesTune' ] = 1.\n",
    "    VARCVDF.loc[ VARCVDF['weightSplineTimesTune'] == np.inf, 'weightSplineTimesTune' ] = 1.\n",
    "    VARCVDF.loc[ VARCVDF['weightSplineTimesTune'] > 100, 'weightSplineTimesTune' ] = 1.\n",
    "    VARCVDF.loc[ np.isnan(VARCVDF['weightSplineTimesTune']) == True, 'weightSplineTimesTune' ] = 1.\n",
    "\n",
    "    # if running nue selections, define all necessary variables\n",
    "    if (NUEVARIABLES):\n",
    "        INTERCEPT = 0.0\n",
    "        SLOPE = 0.83\n",
    "        trk_llr_pid_v = VAR.array('trk_llr_pid_score_v')\n",
    "        trk_energy_proton_v = VAR.array('trk_energy_proton_v')\n",
    "        trk_calo_energy_y_v = VAR.array('trk_calo_energy_y_v')\n",
    "        trk_id = VAR.array('trk_id')-1 # I think we need this -1 to get the right result\n",
    "        trk_llr_pid_v_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_llr_pid_v,trk_id)])\n",
    "        trk_energy_proton_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_energy_proton_v,trk_id)])\n",
    "        trk_calo_energy_y_sel = awkward.fromiter([pidv[tid] if tid<len(pidv) else 9999. for pidv,tid in zip(trk_calo_energy_y_v,trk_id)])\n",
    "        VARCVDF['trkpid'] = trk_llr_pid_v_sel\n",
    "        VARCVDF['protonenergy'] = trk_energy_proton_sel\n",
    "        VARCVDF['trackcaloenergy'] = trk_calo_energy_y_sel\n",
    "        VARCVDF['subcluster'] = VARCVDF['shrsubclusters0'] + VARCVDF['shrsubclusters1'] + VARCVDF['shrsubclusters2']\n",
    "        VARCVDF['trkfit'] = VARCVDF['shr_tkfit_npointsvalid'] / VARCVDF['shr_tkfit_npoints']\n",
    "        VARCVDF['anglediff_Y'] = np.abs(VARCVDF['secondshower_Y_dir']-VARCVDF['shrclusdir2'])\n",
    "        VARCVDF['shr_tkfit_nhits_tot'] = (VARCVDF['shr_tkfit_nhits_Y']+VARCVDF['shr_tkfit_nhits_U']+VARCVDF['shr_tkfit_nhits_V'])\n",
    "        VARCVDF.loc[:,'shr_tkfit_dedx_max'] = VARCVDF['shr_tkfit_dedx_Y']\n",
    "        VARCVDF.loc[(VARCVDF['shr_tkfit_nhits_U']>VARCVDF['shr_tkfit_nhits_Y']),'shr_tkfit_dedx_max'] = VARCVDF['shr_tkfit_dedx_U']\n",
    "        VARCVDF.loc[(VARCVDF['shr_tkfit_nhits_V']>VARCVDF['shr_tkfit_nhits_Y']) & (VARCVDF['shr_tkfit_nhits_V']>VARCVDF['shr_tkfit_nhits_U']),'shr_tkfit_dedx_max'] = VARCVDF['shr_tkfit_dedx_V']\n",
    "        VARCVDF[\"reco_e\"] = (VARCVDF[\"shr_energy_tot_cali\"] + INTERCEPT) / SLOPE + VARCVDF[\"trk_energy_tot\"]   \n",
    "        VARCVDF[\"is_signal\"] = VARCVDF[\"category\"] == 11 \n",
    "    \n",
    "        if (USEBDT == True):\n",
    "            for label, bkg_query in zip(BDT_LABELS, nue_booster.bkg_queries):\n",
    "                with open(ls.pickle_path+'booster_%s_0304_extnumi.pickle' % label, 'rb') as booster_file:\n",
    "                    booster = pickle.load(booster_file)\n",
    "                    VARCVDF[label+\"_score\"] = booster.predict(\n",
    "                        xgb.DMatrix(VARCVDF[TRAINVAR]),\n",
    "                        ntree_limit=booster.best_iteration)\n",
    "    \n",
    "    if (NUMUVARIABLES):\n",
    "        VARCVDF['trk_p_quality_v'] = (VARCVDF['trk_mcs_muon_mom_v']-VARCVDF['trk_range_muon_mom_v'])/VARCVDF['trk_range_muon_mom_v']\n",
    "        VARCVDF['trk_costheta_v'] = VARCVDF['trk_theta_v'].apply(lambda x: np.cos(x))\n",
    "        VARCVDF['reco_nu_e_range'] = VARCVDF[\"trk_energy_muon\"] + (VARCVDF[\"trk_energy_tot\"] - VARCVDF[\"trk_energy\"]) + M_mu\n",
    "        VARCVDF['reco_muon_e_range'] = VARCVDF[\"trk_energy_muon\"] + M_mu\n",
    "                                     \n",
    "    INT = pd.merge(CVDF, VARCVDF, how='inner', on=['identifier'],suffixes=('_CV', '_VAR'))\n",
    "\n",
    "    \n",
    "    print ('intersection for %15s variation has %i events'%(N,INT.shape[0]))\n",
    "    DETSYS_SAMPLE_V.append(INT)\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for this notebook\n",
    "These are here instead of the plotter.py due to the odd way in which the variables need to be defined with the \"_CV\" or \"_VAR\" appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#all the code necessary to make the track-level cuts\n",
    "###################################################\n",
    "def _apply_track_cuts(df,variable,track_cuts,mask,verbose=False):\n",
    "    \"\"\"\n",
    "    returns array with track-level cuts applied\n",
    "    \n",
    "    need to do this fancy business with the apply function to make masks\n",
    "    \"\"\"\n",
    "    for (var,op,val) in track_cuts:\n",
    "        if type(op) == list:\n",
    "            #this means treat two conditions in an 'or' fashion\n",
    "            or_mask1 = df[var].apply(lambda x: eval(\"x{}{}\".format(op[0],val[0])))#or condition 1\n",
    "            or_mask2 = df[var].apply(lambda x: eval(\"x{}{}\".format(op[1],val[1])))#or condition 2\n",
    "            mask *= (or_mask1 + or_mask2) #just add the booleans\n",
    "        else:\n",
    "            mask *= df[var].apply(lambda x: eval(\"x{}{}\".format(op,val))) #layer on each cut mask\n",
    "    VARS = (df[variable]*mask).apply(lambda x: x[x != False]) #apply mask\n",
    "    if verbose: print(\"before cleaning: \\n{}\".format(VARS))\n",
    "    VARS = VARS[VARS.apply(lambda x: len(x) > 0)] #clean up empty slices\n",
    "    #fix list comprehension issue for non '_v' variables\n",
    "    if \"_v_\" not in variable:\n",
    "        VARS = VARS.apply(lambda x: x[0])\n",
    "        \n",
    "    if verbose:\n",
    "        print(variable)\n",
    "        print(\"before mask: {}\".format(df[variable]))\n",
    "        print(\"after mask: {}\".format(VARS))\n",
    "        try:\n",
    "            print(\"masking \", VARS[VARS.apply(lambda x: len(x) <= 0)])\n",
    "        except:\n",
    "            print(\"not masking\")\n",
    "\n",
    "    return VARS, mask\n",
    "\n",
    "def _select_longest(df, VARS, mask):\n",
    "    if \"_CV\" in VARS.name:\n",
    "        trk_lens = (df['trk_len_v_CV']*mask).apply(lambda x: x[x != False])#apply mask to track lengths\n",
    "    elif \"_VAR\" in VARS.name:\n",
    "        trk_lens = (df['trk_len_v_VAR']*mask).apply(lambda x: x[x != False])#apply mask to track lengths\n",
    "    else:\n",
    "        raise ValueError(\"Improper name of VARS: {}\".format(VARS.name))\n",
    "    trk_lens = trk_lens[trk_lens.apply(lambda x: len(x) > 0)]#clean up empty slices\n",
    "    longest_mask = trk_lens.apply(lambda x: x == x[list(x).index(max(x))])#identify longest\n",
    "    VARS = (VARS*longest_mask).apply(lambda x: x[x!=False])#apply mask\n",
    "    VARS = VARS[VARS.apply(lambda x: len(x) > 0)]#clean up empty slices\n",
    "    if len(VARS.iloc[0]) == 1:\n",
    "        VARS = VARS.apply(lambda x: x[0] if len(x)>0 else -999)#expect values, not lists, for each event\n",
    "    else:\n",
    "        raise ValueError(\n",
    "        \"There are more than one longest track per slice\")\n",
    "\n",
    "    return VARS, longest_mask\n",
    "\n",
    "def _selection(variable, sample, query=\"selected==1\", extra_cut='', \n",
    "               track_cuts=None, select_longest=True,verbose=False):\n",
    "    \"\"\"\n",
    "    variable: string, about which info to be returned\n",
    "    sample: dataframe, to be queried\n",
    "    query: string, event-level queries\n",
    "    extra_cut: string, just another cut in addition to query\n",
    "    select_longest: bool, will select longest track in each slice after cuts\n",
    "        should be on when variable is track-level\n",
    "    fix: string, should be \"_CV\" or \"_VAR\"\n",
    "        on which sample to apply the cuts\n",
    "    return_fix: string, should be \"_CV\" or \"_VAR\"\n",
    "        which sample to return\n",
    "    \n",
    "    Returns an array of track/event variables that pass cuts\n",
    "    \"\"\"\n",
    "    query += extra_cut\n",
    "        \n",
    "    df = sample.copy().query(query)\n",
    "    #start dealing with the track variables\n",
    "    if track_cuts is not None:\n",
    "        track_cuts_mask = df[variable].apply(lambda x: x == x) #all-True mask\n",
    "        VARS, track_cuts_mask = _apply_track_cuts(df,variable,track_cuts,track_cuts_mask,verbose=verbose)#VARS is cleaned of empty frames\n",
    "    else:\n",
    "        VARS = df[variable]#VARS is not clean of empty frames\n",
    "\n",
    "    if \"_v_\" in variable and select_longest:\n",
    "        VARS, longest_mask = _select_longest(df, VARS, track_cuts_mask,fix=fix)\n",
    "    \n",
    "    return VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COVARIANCE(n_cv,n_var):\n",
    "    cov = np.empty([len(n_cv), len(n_cv)])\n",
    "    cov.fill(0)\n",
    "\n",
    "    for i in range(len(n_cv)):\n",
    "        for j in range(len(n_cv)):\n",
    "            cov[i][j] += (n_var[i] - n_cv[i])*(n_var[j] - n_cv[j])\n",
    "\n",
    "    frac_cov = np.empty([len(n_cv), len(n_cv)])\n",
    "    corr = np.empty([len(n_cv), len(n_cv)])\n",
    "\n",
    "    for i in range(len(n_cv)):\n",
    "        for j in range(len(n_cv)):\n",
    "            frac_cov[i][j] =  cov[i][j] / (n_cv[i] * n_cv[j])\n",
    "            corr[i][j] = cov[i][j] / np.sqrt(cov[i][i] * cov[j][j])\n",
    "    return cov,frac_cov,corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SliceQuery(APP):\n",
    "    return 'nslice_%s == 1'%APP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "#plt.rcParams.update({'font.size': 14})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "# Make sheet-o-plots\n",
    "# Hist_LYA | frac_cov_LYA\n",
    "# Hist_SCE | frac_cov_SCE\n",
    "# ...\n",
    "##################################\n",
    "importlib.reload(ls)\n",
    "\n",
    "#potentially useful values\n",
    "AVx = [-1.55,254.8]\n",
    "AVy = [-115.53, 117.47]\n",
    "AVz = [0.1, 1036.9]\n",
    "M_mu = 0.105 #GeV/c\n",
    "\n",
    "#choose what you want to show\n",
    "VARIABLE = 'reco_nu_e_range'\n",
    "TITLE = r'Reco Range-Based $\\nu$ Energy [GeV]'\n",
    "BINEDGES = np.linspace(0.15,1.55,15)\n",
    "\n",
    "QUERY_CV, track_cuts_CV = NUMUQuery(\"CV\")\n",
    "QUERY_VAR, track_cuts_VAR = NUMUQuery(\"VAR\")\n",
    "#QUERY_CV, track_cuts_CV = 'nslice_CV == 1', None\n",
    "#QUERY_VAR, track_cuts_VAR = 'nslice_VAR == 1', None\n",
    "#track_cuts_CV = [track_cuts_CV[0]]\n",
    "#track_cuts_VAR = [track_cuts_VAR[0]]\n",
    "print(QUERY_CV,QUERY_VAR)\n",
    "print(track_cuts_CV,track_cuts_VAR)\n",
    "\n",
    "SAVEFIG = True\n",
    "tag = \"fullsel\"\n",
    "\n",
    "#fig = plt.figure(figsize=(20,3*len(DETVAR_N_V)))\n",
    "fig = plt.figure(figsize=(15,50))\n",
    "gs = fig.add_gridspec(10, 20)\n",
    "for idx,df_perm in enumerate(DETSYS_SAMPLE_V):\n",
    "    df = df_perm.copy()\n",
    "    print(\"starting {}...\".format(DETVAR_N_V[idx]))\n",
    "    \n",
    "    VARS_CV = _selection(VARIABLE+'_CV',\n",
    "                        df,QUERY_CV,\n",
    "                        track_cuts = track_cuts_CV,\n",
    "                        select_longest = True)\n",
    "    WEIGHTS_CV = _selection('weightSplineTimesTune_CV',\n",
    "                            df,QUERY_CV,\n",
    "                            track_cuts = track_cuts_CV,\n",
    "                            select_longest = True)\n",
    "    \n",
    "    VARS_VAR = _selection(VARIABLE+'_VAR',\n",
    "                            df,QUERY_VAR,\n",
    "                            track_cuts = track_cuts_VAR,\n",
    "                            select_longest = True,\n",
    "                            verbose=False)\n",
    "    WEIGHTS_VAR = _selection('weightSplineTimesTune_VAR',\n",
    "                            df,QUERY_VAR,\n",
    "                            track_cuts = track_cuts_VAR,\n",
    "                            select_longest = True,\n",
    "                            verbose=False)\n",
    "\n",
    "    #######################################\n",
    "    axis = fig.add_subplot(gs[idx, 0:11])\n",
    "    ################################\n",
    "    # CV-VAR histogram comparison\n",
    "    #get queried arrays of the variable\n",
    "    #get number of entries in each bin for each sample. and plot hists\n",
    "    n_cv, bins, p = axis.hist(VARS_CV ,bins=BINEDGES,histtype='step',lw=2,color='k',label='CV',weights=WEIGHTS_CV)\n",
    "    n_var, bins, p = axis.hist(VARS_VAR,bins=BINEDGES,histtype='step',lw=2,color='r',label='var : %s'%DETVAR_N_V[idx],weights=WEIGHTS_VAR)\n",
    "\n",
    "    bc = 0.5*(bins[1:]+bins[:-1]) #bin centers\n",
    "\n",
    "    cov,frac_cov,corr = COVARIANCE(n_cv,n_var) #calculate various matrices\n",
    "    error = np.sqrt(np.diag(frac_cov)) #systematic error is this\n",
    "    #incorporate errors from fractional covariance matrix\n",
    "    axis.bar(bc,height=2*(error)*n_cv,bottom=n_cv-(error)*n_cv,width=bc[1]-bc[0], \n",
    "            edgecolor='gray',color='None',lw=2)\n",
    "\n",
    "    axis.set_xlabel(TITLE)\n",
    "    axis.set_ylabel('Num. Entries',fontsize=16)\n",
    "    #plt.ylim(0,plt.gca().get_ylim()[1]*1.5)\n",
    "    axis.legend(fontsize=15,loc=\"best\")\n",
    "    #plt.title(SAMPLE)\n",
    "\n",
    "    ########################################\n",
    "    axis = fig.add_subplot(gs[idx, 12:])\n",
    "    #####################################\n",
    "    # Fractional Covariance\n",
    "    pos = axis.imshow(frac_cov, origin='lower', cmap='viridis')#,vmin=-0.1,vmax=0.1) #be consistent when comparing multiple plots\n",
    "    #print values onto the plot\n",
    "    # Limits for the extent\n",
    "    x_start = 0\n",
    "    x_end = len(n_cv)#-1\n",
    "    y_start = 0\n",
    "    y_end = len(n_cv)#-1\n",
    "    size = len(n_cv)#-1\n",
    "    jump_x = (x_end - x_start) / (2.0 * size)\n",
    "    jump_y = (y_end - y_start) / (2.0 * size)\n",
    "    x_positions = np.linspace(start=x_start, stop=x_end, num=size, endpoint=False)\n",
    "    y_positions = np.linspace(start=y_start, stop=y_end, num=size, endpoint=False)\n",
    "    for x_index, x in enumerate(x_positions):\n",
    "        #for x_index, x in enumerate(x_positions):\n",
    "        ERR = frac_cov[x_index, x_index]\n",
    "        #label = \"{:.2f}\\n{} {}\".format(100.*np.sqrt(ERR),int(round(n_cv[x_index])),int(round(n_var[x_index])))\n",
    "        label = \"{:.2f}\".format(100.*np.sqrt(ERR))\n",
    "        text_x = x #+ jump_x\n",
    "        text_y = x #+ jump_y\n",
    "        if (np.abs(ERR) > 0.05):\n",
    "            axis.text(text_x, text_y, label, color='black', ha='center', va='center')#,fontsize=8)\n",
    "        else:\n",
    "            axis.text(text_x, text_y, label, color='white', ha='center', va='center')#,fontsize=8)\n",
    "    fig.colorbar(pos, ax=axis)\n",
    "    axis.set_ylabel(\"Bin number\")\n",
    "    axis.set_xlabel(\"Bin number\")\n",
    "    axis.set_title(VARIABLE)\n",
    "            \n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if SAVEFIG: \n",
    "    SAVEPATH = ls.main_path+'detsys\\\\'\n",
    "    try:\n",
    "        SAVEPATH += tag+'\\\\'\n",
    "        tag = \"_\" + tag\n",
    "    except:\n",
    "        SAVEPATH += 'plots\\\\'\n",
    "        \n",
    "    if not os.path.exists(SAVEPATH):\n",
    "        os.makedirs(SAVEPATH)\n",
    "    fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}{}.pdf\".format(VARIABLE,tag))\n",
    "    \n",
    "SAVEFIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Output .csv files of the sqrt(diag) frac_cov errors\n",
    "######################################################\n",
    "\n",
    "VARIABLE = 'reco_nu_e_range'\n",
    "BINEDGES = np.linspace(0.15,1.55,15)\n",
    "\n",
    "QUERY_CV, track_cuts_CV = NUMUQuery(\"CV\")\n",
    "QUERY_VAR, track_cuts_VAR = NUMUQuery(\"VAR\")\n",
    "\n",
    "tag = \"fullsel\"\n",
    "SAVEFIG = True ######check it\n",
    "PICKLE = True\n",
    "SAVEPATH = ls.main_path+'detsys\\\\csv\\\\'\n",
    "PICKLEPATH = ls.main_path+ls.pickle_path\n",
    "\n",
    "if tag: SAVEPATH += tag+'\\\\'\n",
    "if not os.path.exists(SAVEPATH):\n",
    "    os.makedirs(SAVEPATH)\n",
    "    \n",
    "#make main error dataframe\n",
    "#bin range labels for the rows\n",
    "index_labels = []\n",
    "for b,bin_edge in enumerate(BINEDGES[:-1]):\n",
    "    index_labels.append(\"{}-{}\".format(round(bin_edge,2),round(BINEDGES[b+1],2)))\n",
    "#output dataframe\n",
    "ERRS_DF = pd.DataFrame(index=index_labels)\n",
    "\n",
    "#loop through all samples\n",
    "for i,df_perm in enumerate(DETSYS_SAMPLE_V):\n",
    "    print(\"starting {}...\".format(DETVAR_N_V[i]))   \n",
    "    df = df_perm.copy()\n",
    "    #make cuts\n",
    "    VARS_CV = _selection(VARIABLE+\"_CV\",\n",
    "                        df,QUERY_CV,\n",
    "                        track_cuts = track_cuts_CV,\n",
    "                        select_longest = True)\n",
    "    VARS_VAR = _selection(VARIABLE+\"_VAR\",\n",
    "                        df,QUERY_VAR,\n",
    "                        track_cuts = track_cuts_VAR,\n",
    "                        select_longest = True)\n",
    "    #bin-related things\n",
    "    bc = 0.5*(BINEDGES[1:]+BINEDGES[:-1])\n",
    "    n_cv,_ = np.histogram(VARS_CV,bins=BINEDGES)\n",
    "    n_var,_ = np.histogram(VARS_VAR,bins=BINEDGES)\n",
    "    #make error calculations\n",
    "    cov,frac_cov,corr = COVARIANCE(n_cv,n_var)\n",
    "    error = np.sqrt(np.diag(frac_cov))\n",
    "    #layer these results onto the dataframe\n",
    "    ERRS_DF[DETVAR_N_V[i]] = error\n",
    "    \n",
    "#calculated columns are quadrature sums of columns\n",
    "samples = list(ERRS_DF.keys())\n",
    "ERRS_DF['sum'] = np.sqrt((ERRS_DF[samples]**2).sum(axis=1))\n",
    "samples.remove('Recomb')\n",
    "ERRS_DF['sum_noRecomb'] = np.sqrt((ERRS_DF[samples]**2).sum(axis=1))\n",
    "samples.remove('WireMod dEdX')\n",
    "samples.append('Recomb')\n",
    "ERRS_DF['sum_nodEdX'] = round(np.sqrt((ERRS_DF[samples]**2).sum(axis=1)),2)\n",
    "ERRS_DF = np.around(ERRS_DF, decimals=3) #clean up\n",
    "\n",
    "if SAVEFIG: \n",
    "    ERRS_DF.to_csv(SAVEPATH+'{}_{}_{}.csv'.format(VARIABLE,date_time,tag))\n",
    "    \n",
    "if PICKLE:\n",
    "    ERRS_DF.to_pickle(PICKLEPATH+'{}_{}_{}.pickle'.format(VARIABLE,date_time,tag))\n",
    "    \n",
    "SAVEFIG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Output latex table code\n",
    "###################################\n",
    "indices = list(ERRS_DF.index)\n",
    "header = ''\n",
    "headers = []\n",
    "for key in ERRS_DF.keys():\n",
    "    header += key + ' & '\n",
    "    headers.append(key)\n",
    "print(header[:-2])\n",
    "for i,index in enumerate(indices):\n",
    "    row = index\n",
    "    for header in headers:\n",
    "        row += ' & {}'.format(ERRS_DF[header][i])\n",
    "    row += ' \\\\\\\\'\n",
    "    print(row)\n",
    "    \n",
    "    \n",
    "#ERRS_DF.to_csv(SAVEPATH+'{}_{}_{}.csv'.format(VARIABLE,date_time,tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Make many plots about one VAR sample\n",
    "########################################\n",
    "importlib.reload(ls)\n",
    "\n",
    "#figure out what cuts\n",
    "QUERY, track_cuts = NUMUQuery()  \n",
    "track_cuts = None\n",
    "\n",
    "#setup input\n",
    "#single variable input\n",
    "variables = ['reco_nu_e_range']\n",
    "titles = ['Reconstructed Range-Based Neutrino Energy [GeV]']\n",
    "binss = [np.linspace(M_mu+0.01,1.55,26)]\n",
    "#multivariate input\n",
    "variables = ['reco_nu_e_range', 'reco_muon_e_range',\n",
    "            'trk_llr_pid_score_v',\n",
    "            'reco_nu_vtx_sce_x','reco_nu_vtx_sce_y','reco_nu_vtx_sce_z']\n",
    "titles = ['Reconstructed Neutrino Energy [GeV]','Reconstructed Muon Energy [GeV]',\n",
    "         'Log-Likelihood PID Score',\n",
    "          \"Reconstructed Vertex X [cm]\",\"Reconstructed Vertex Y [cm]\",\"Reconstructed Vertex Z [cm]\"]\n",
    "binss = [np.linspace(0.15,1.55,15),np.linspace(0.15,1.2,15),\n",
    "         np.linspace(-1,1,26),\n",
    "         np.linspace(FVx[0],FVx[1],26),np.linspace(FVy[0],FVy[1],26),np.linspace(FVz[0],FVz[1],26)]\n",
    "\n",
    "#select variation sample\n",
    "idx = DETVAR_N_V.index('SCE') #just looking at one variation \n",
    "df = DETSYS_SAMPLE_V[idx].copy()  #avoid making changed to source\n",
    "tag = None\n",
    "\n",
    "print(\"QUERY: {}\".format(QUERY))\n",
    "print(\"track_cuts: {}\".format(track_cuts))\n",
    "for VARIABLE,TITLE,BINS in  zip(variables,titles,binss):    \n",
    "    #loop though variables\n",
    "    print(\"{}\\t{}\\t{}\".format(VARIABLE,TITLE,BINS))\n",
    "    \n",
    "    SAVEPATH = ls.main_path+\"detsys\\\\plots\\\\\"\n",
    "    if tag:\n",
    "        SAVEPATH += tag[1:]+'\\\\'\n",
    "    if not os.path.exists(SAVEPATH):\n",
    "        os.makedirs(SAVEPATH)\n",
    "\n",
    "        \n",
    "    ################################\n",
    "    # CV-VAR histogram comparison\n",
    "    fig = plt.figure(figsize=(7,5))\n",
    "    \n",
    "    #get queried array of the variable\n",
    "    VARS_CV = _selection(VARIABLE,\n",
    "                        df,QUERY,\n",
    "                        track_cuts = track_cuts,\n",
    "                        select_longest = True,\n",
    "                        fix = '_CV')\n",
    "    VARS_VAR = _selection(VARIABLE,\n",
    "                        df,QUERY,\n",
    "                        track_cuts = track_cuts,\n",
    "                        select_longest = True,\n",
    "                        fix = '_VAR')\n",
    "\n",
    "    WEIGHTS_CV = _selection('weightSplineTimesTune',\n",
    "                            df,QUERY,\n",
    "                            track_cuts = track_cuts,\n",
    "                            select_longest = True,\n",
    "                            fix = '_CV')\n",
    "    WEIGHTS_VAR = _selection('weightSplineTimesTune',\n",
    "                            df,QUERY,\n",
    "                            track_cuts = track_cuts,\n",
    "                            select_longest = True,\n",
    "                            fix = '_VAR')\n",
    "\n",
    "    #get number of entries in each bin for each sample. and plot hists\n",
    "    n_cv, bins, p = plt.hist(VARS_CV ,bins=BINS,histtype='step',lw=2,color='k',\\\n",
    "             label='CV',weights=WEIGHTS_CV)\n",
    "    n_var, bins, p = plt.hist(VARS_VAR,bins=BINS,histtype='step',lw=2,color='r',\\\n",
    "             label='var : %s'%DETVAR_N_V[idx],weights=WEIGHTS_VAR)\n",
    "\n",
    "    bc = 0.5*(bins[1:]+bins[:-1]) #bin centers\n",
    "\n",
    "    cov,frac_cov,corr = COVARIANCE(n_cv,n_var) #calculate various matrices\n",
    "    error = np.sqrt(np.diag(frac_cov)) #systematic error is this\n",
    "    #incorporate errors from fractional covariance matrix\n",
    "    plt.bar(bc,height=2*(error)*n_cv,bottom=n_cv-(error)*n_cv,width=bc[1]-bc[0], \n",
    "            edgecolor='gray',color='None',lw=2)\n",
    "    \n",
    "    plt.xlabel(TITLE)\n",
    "    #plt.ylabel('Num. Entries',fontsize=16)\n",
    "    #plt.ylim(0,plt.gca().get_ylim()[1]*1.5)\n",
    "    plt.legend(fontsize=15,loc=\"best\")\n",
    "    plt.title(SAMPLE)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG: fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}{}.pdf\".format(DETVAR_N_V[idx],tag))\n",
    "\n",
    "    #########################################\n",
    "    #Show the variance of the variable\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    \n",
    "    #apply _CV cuts only for equal sized arrays\n",
    "    VARS_VAR_CV = _selection(VARIABLE,\n",
    "                        df,QUERY,\n",
    "                        track_cuts = track_cuts,\n",
    "                        select_longest = True,\n",
    "                        fix = '_CV',\n",
    "                        return_fix = '_VAR')\n",
    "    #trim the out of range bits\n",
    "    mask_inrange = VARS_CV<BINS[-1]\n",
    "    mask_inrange *= VARS_CV>BINS[0]\n",
    "    mask_inrange *= VARS_VAR_CV<BINS[-1]\n",
    "    mask_inrange *= VARS_VAR_CV>BINS[0]\n",
    "    #plot variance\n",
    "    plt.hist2d(VARS_CV[mask_inrange],VARS_VAR_CV[mask_inrange],bins=(BINS.size-1,BINS.size-1),norm=LogNorm())\n",
    "    plt.xlabel('%s [CV]'%TITLE)\n",
    "    plt.ylabel('%s [VAR]'%TITLE)\n",
    "    plt.title('variation : %s'%DETVAR_N_V[idx],fontsize=16)\n",
    "    plt.plot(BINS,BINS,'r--',lw=2)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    if SAVEFIG: fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}_variation{}.pdf\".format(DETVAR_N_V[idx],tag))\n",
    "\n",
    "    ####################################\n",
    "    # Make covariance plots\n",
    "    fig, axes = plt.subplots(2, 1,figsize=(10,12))\n",
    "    #start with fractional covariance\n",
    "    ax = axes[0]\n",
    "    #plot fractional covariance\n",
    "    pos = ax.imshow(frac_cov, origin='lower', cmap='viridis')#,vmin=-0.1,vmax=0.1) #be consistent when comparing multiple plots\n",
    "        # Limits for the extent\n",
    "        x_start = 0\n",
    "        x_end = len(n_cv)#-1\n",
    "        y_start = 0\n",
    "        y_end = len(n_cv)#-1\n",
    "        size = len(n_cv)#-1\n",
    "        jump_x = (x_end - x_start) / (2.0 * size)\n",
    "        jump_y = (y_end - y_start) / (2.0 * size)\n",
    "        x_positions = np.linspace(start=x_start, stop=x_end, num=size, endpoint=False)\n",
    "        y_positions = np.linspace(start=y_start, stop=y_end, num=size, endpoint=False)\n",
    "        for x_index, x in enumerate(x_positions):\n",
    "            #for x_index, x in enumerate(x_positions):\n",
    "            ERR = frac_cov[x_index, x_index]\n",
    "            #label = \"{:.2f}\\n{} {}\".format(100.*np.sqrt(ERR),int(round(n_cv[x_index])),int(round(n_var[x_index])))\n",
    "            label = \"{:.2f}\".format(100.*np.sqrt(ERR))\n",
    "            text_x = x #+ jump_x\n",
    "            text_y = x #+ jump_y\n",
    "            if (np.abs(ERR) > 0.05):\n",
    "                ax.text(text_x, text_y, label, color='black', ha='center', va='center',fontsize=8)\n",
    "            else:\n",
    "                ax.text(text_x, text_y, label, color='white', ha='center', va='center',fontsize=8)\n",
    "\n",
    "        fig.colorbar(pos, ax=ax)\n",
    "        #start on correlation matrix\n",
    "        ax = axes[1]\n",
    "        ax.set_title(\"Correlation matrix\")\n",
    "        pos = ax.imshow(corr, origin='lower')\n",
    "        ax.set_ylabel(\"Bin number\")\n",
    "        ax.set_xlabel(\"Bin number\")\n",
    "        fig.colorbar(pos, ax = ax)\n",
    "        \n",
    "        if SAVEFIG: fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}_corrs{}.pdf\".format(DETVAR_N_V[idx],tag))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Make lots of plots over all the variations\n",
    "# Loop over several variables too\n",
    "#############################################\n",
    "importlib.reload(ls)\n",
    "\n",
    "#have to edit code to just accept one of these at a time\n",
    "VARIABLE = \"trk_p_quality_v\"\n",
    "TITLE = \"MCS Consistency\"\n",
    "BINS = np.linspace(-2,5,15)\n",
    "\n",
    "VARIABLE = \"trk_len_v\"\n",
    "TITLE = \"Track Length, Muon Candidate [cm]\"\n",
    "BINS = np.linspace(0,500,20)\n",
    "\n",
    "VARIABLE = \"topological_score\"\n",
    "TITLE = \"Topological Score\"\n",
    "BINS = np.linspace(0,1,25)\n",
    "\n",
    "#do this for single var...below is multivar\n",
    "variables = [VARIABLE]\n",
    "titles = [TITLE]\n",
    "binss = [BINS]\n",
    "\n",
    "\n",
    "# multivariate study\n",
    "\"\"\"\n",
    "variables = ['reco_nu_e_range', 'reco_muon_e_range',\n",
    "            'trk_llr_pid_score_v',\n",
    "            ]\n",
    "titles = ['Reconstructed Neutrino Energy [GeV]','Reconstructed Muon Energy [GeV]',\n",
    "         'Log-Likelihood PID Score',\n",
    "          ]\n",
    "binss = [np.linspace(0.15,1.55,15),np.linspace(0.15,1.2,15),\n",
    "         np.linspace(-1,1,26),\n",
    "         ]\n",
    "\"\"\"\n",
    "\n",
    "tag = '_SCEstudy' #will probably help define save path\n",
    "\n",
    "for VARIABLE,TITLE,BINS in  zip(variables,titles,binss):\n",
    "    print(\"\\n\\nDOING {} NOW!!!!!!\\n\".format(VARIABLE))\n",
    "    \n",
    "    SAVEPATH = ls.main_path+\"detsys\\\\plots\\\\\"\n",
    "    SAVEPATH += '{}\\\\'.format(VARIABLE)\n",
    "    if tag:\n",
    "        SAVEPATH += tag[1:]+'\\\\'\n",
    "    if not os.path.exists(SAVEPATH):\n",
    "        os.makedirs(SAVEPATH)\n",
    "\n",
    "    for i,df_perm in enumerate(DETSYS_SAMPLE_V):\n",
    "        print(\"starting {}...\".format(DETVAR_N_V[i]))\n",
    "        df = df_perm.copy()\n",
    "        idx = i#+6 #\n",
    "\n",
    "        ################################\n",
    "        # CV-VAR histogram comparison \n",
    "        fig = plt.figure(figsize=(7,5))\n",
    "        #get queried array of the variable\n",
    "        VARS_CV = _selection(VARIABLE,\n",
    "                            df,QUERY,\n",
    "                            track_cuts = track_cuts,\n",
    "                            select_longest = True,\n",
    "                            fix = '_CV')\n",
    "        VARS_VAR = _selection(VARIABLE,\n",
    "                            df,QUERY,\n",
    "                            track_cuts = track_cuts,\n",
    "                            select_longest = True,\n",
    "                            fix = '_VAR')\n",
    "        \n",
    "        WEIGHTS_CV = _selection('weightSplineTimesTune',\n",
    "                                df,QUERY,\n",
    "                                track_cuts = track_cuts,\n",
    "                                select_longest = True,\n",
    "                                fix = '_CV')\n",
    "        WEIGHTS_VAR = _selection('weightSplineTimesTune',\n",
    "                                df,QUERY,\n",
    "                                track_cuts = track_cuts,\n",
    "                                select_longest = True,\n",
    "                                fix = '_VAR')\n",
    "        #get n entries per bin\n",
    "        n_cv, bins, p = plt.hist(VARS_CV ,bins=BINS,histtype='step',lw=2,color='k',\\\n",
    "                 label='CV',weights=WEIGHTS_CV)\n",
    "        n_var, bins, p = plt.hist(VARS_VAR,bins=BINS,histtype='step',lw=2,color='r',\\\n",
    "                 label='var : %s'%DETVAR_N_V[idx],weights=WEIGHTS_VAR)\n",
    "        #calc bin centers\n",
    "        bc = 0.5*(bins[1:]+bins[:-1])\n",
    "        #make covariance and correlation calculations\n",
    "        cov,frac_cov,corr = COVARIANCE(n_cv,n_var)\n",
    "        #get and plot the systematic error with these cuts + variable\n",
    "        error = np.sqrt(np.diag(frac_cov))\n",
    "        plt.bar(bc,height=2*(error)*n_cv,bottom=n_cv-(error)*n_cv,width=bc[1]-bc[0] ,\\\n",
    "                edgecolor='gray',color='None',lw=2)\n",
    "        plt.xlabel(TITLE)\n",
    "        plt.ylabel('Num. Entries')\n",
    "        plt.ylim(0,plt.gca().get_ylim()[1]*1.5)\n",
    "        plt.legend(fontsize=16,loc=1)\n",
    "        plt.show()\n",
    "        #fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}{}.pdf\".format(DETVAR_N_V[idx],tag))\n",
    "\n",
    "        #####################################################\n",
    "        #Show the variance of the variable\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        #apply _CV cuts only to get equal sized arrays\n",
    "        VARS_VAR_CV = _selection(VARIABLE,\n",
    "                            df,QUERY,\n",
    "                            track_cuts = track_cuts,\n",
    "                            select_longest = True,\n",
    "                            fix = '_CV',\n",
    "                            return_fix = '_VAR') #VAR events that correspond to CV events that pass all cuts\n",
    "        mask_inrange = VARS_CV<BINS[-1]\n",
    "        mask_inrange *= VARS_CV>BINS[0]\n",
    "        mask_inrange *= VARS_VAR_CV<BINS[-1]\n",
    "        mask_inrange *= VARS_VAR_CV>BINS[0]\n",
    "        plt.hist2d(VARS_CV[mask_inrange],VARS_VAR_CV[mask_inrange],bins=(BINS.size-1,BINS.size-1))#,norm=LogNorm())\n",
    "        plt.xlabel('%s [CV]'%TITLE)\n",
    "        plt.ylabel('%s [VAR]'%TITLE)\n",
    "        plt.title('variation : %s'%DETVAR_N_V[idx],fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.plot(BINS,BINS,'r--',lw=2)\n",
    "        plt.colorbar()\n",
    "        #fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}_variation{}.pdf\".format(DETVAR_N_V[idx],tag))\n",
    "\n",
    "        ########################################\n",
    "        # do correlation and frac-covariance plot\n",
    "        fig, axes = plt.subplots(2, 1,figsize=(10,12))\n",
    "        #start with fractional covariance\n",
    "        ax = axes[0]\n",
    "        pos = ax.imshow(frac_cov, origin='lower', cmap='viridis')#,vmin=-0.1,vmax=0.1)\n",
    "        ax.set_title(\"Fractional covariance matrix : %s\"%DETVAR_N_V[idx])\n",
    "        ax.set_ylabel(\"Bin number\")\n",
    "        ax.set_xlabel(\"Bin number\")\n",
    "        \n",
    "        # Add text for errors on diagonal of frac_covariance\n",
    "        # Limits for the extent\n",
    "        x_start = 0\n",
    "        x_end = len(n_cv)#-1\n",
    "        y_start = 0\n",
    "        y_end = len(n_cv)#-1\n",
    "        size = len(n_cv)#-1\n",
    "        jump_x = (x_end - x_start) / (2.0 * size)\n",
    "        jump_y = (y_end - y_start) / (2.0 * size)\n",
    "        x_positions = np.linspace(start=x_start, stop=x_end, num=size, endpoint=False)\n",
    "        y_positions = np.linspace(start=y_start, stop=y_end, num=size, endpoint=False)\n",
    "        for x_index, x in enumerate(x_positions):\n",
    "            #for x_index, x in enumerate(x_positions):\n",
    "            ERR = frac_cov[x_index, x_index]\n",
    "            #label = \"{:.2f}\\n{} {}\".format(100.*np.sqrt(ERR),int(round(n_cv[x_index])),int(round(n_var[x_index])))\n",
    "            label = \"{:.2f}\".format(100.*np.sqrt(ERR))\n",
    "            text_x = x #+ jump_x\n",
    "            text_y = x #+ jump_y\n",
    "            if (np.abs(ERR) > 0.05):\n",
    "                ax.text(text_x, text_y, label, color='black', ha='center', va='center',fontsize=8)\n",
    "            else:\n",
    "                ax.text(text_x, text_y, label, color='white', ha='center', va='center',fontsize=8)\n",
    "\n",
    "        fig.colorbar(pos, ax=ax)\n",
    "        #start on correlation matrix\n",
    "        ax = axes[1]\n",
    "        ax.set_title(\"Correlation matrix\")\n",
    "        pos = ax.imshow(corr, origin='lower')\n",
    "        ax.set_ylabel(\"Bin number\")\n",
    "        ax.set_xlabel(\"Bin number\")\n",
    "        fig.colorbar(pos, ax = ax)\n",
    "        \n",
    "        #fig.savefig(SAVEPATH+VARIABLE+\"_\"+date_time+\"_{}_corrs{}.pdf\".format(DETVAR_N_V[idx],tag))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David's stuff I shouldn't mess with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose here which selection to apply\n",
    "\n",
    "#SELECTION = \"1eNpBox\"\n",
    "#QUERY_CV = NpBoxCutQuery('CV',BDT=False)\n",
    "#QUERY_VAR = NpBoxCutQuery('VAR',BDT=False)\n",
    "\n",
    "#SELECTION = \"1eNpBox\"\n",
    "#QUERY_CV = NpBoxCutQuery('CV',BDT=USEBDT)\n",
    "#QUERY_VAR = NpBoxCutQuery('VAR',BDT=USEBDT)\n",
    "\n",
    "#SELECTION = \"Slice\"\n",
    "#QUERY_CV = SliceQuery('CV')\n",
    "#QUERY_VAR = SliceQuery('VAR')\n",
    "\n",
    "SELECTION = \"Pi0Selection\"\n",
    "QUERY_CV = Pi0Query('CV')\n",
    "QUERY_VAR = Pi0Query('VAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def gauss(x,mu,sigma,A):\n",
    "    norm = A/(np.sqrt(2*np.pi)*sigma)\n",
    "    exp  = np.exp(-((x-mu)**2)/(2*sigma*sigma))\n",
    "    return norm * exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VARIABLE = \"pi0_dedx1_fit_Y\"\n",
    "TITLE = 'trk PID'\n",
    "BINS = np.linspace(1,10,41)\n",
    "print (BINS)\n",
    "\n",
    "DIAG_VAL_QUAD_V = [] # quadrature sum\n",
    "DIAG_VAL_V_V = [] # vector for each variation\n",
    "for b in range(len(BINS)-1):\n",
    "    DIAG_VAL_QUAD_V.append(0)\n",
    "\n",
    "for i,df in enumerate(DETSYS_SAMPLE_V):\n",
    "    \n",
    "    DIAG_VAL = []\n",
    "\n",
    "    dfsub = df\n",
    "    \n",
    "    dfsub_CV  = df.query(QUERY_CV)\n",
    "    dfsub_VAR = df.query(QUERY_VAR)\n",
    "\n",
    "    idx = i#+6\n",
    "    \n",
    "    SCALE = 1.0 #(1.01e21)/POT_V[i]\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    \n",
    "    n_cv, bins, p = plt.hist(dfsub_CV['%s_CV'%VARIABLE] ,bins=BINS,histtype='step',lw=2,color='k',\\\n",
    "             label='CV',weights=dfsub_CV['weightSplineTimesTune_CV']*SCALE)\n",
    "    \n",
    "    n_var, bins, p = plt.hist(dfsub_VAR['%s_VAR'%VARIABLE],bins=BINS,histtype='step',lw=2,color='r',\\\n",
    "             label='%s'%DETVAR_N_V[idx],weights=dfsub_VAR['weightSplineTimesTune_VAR']*SCALE)\n",
    "    \n",
    "    bc = 0.5*(bins[1:]+bins[:-1])\n",
    "    \n",
    "    cov,frac_cov,corr = COVARIANCE(n_cv,n_var)\n",
    "    \n",
    "    error = np.sqrt(np.diag(frac_cov))\n",
    "    \n",
    "    plt.xlabel(TITLE)\n",
    "    #plt.ylabel('Num. Entries',fontsize=16)\n",
    "    #plt.ylim(0,plt.gca().get_ylim()[1]*1.5)\n",
    "    plt.legend(fontsize=15,loc=\"best\")\n",
    "    plt.title(SAMPLE)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if (SAVEFIG):\n",
    "        fig.savefig(ls.main_path+\"plots/\"+VARIABLE+\"_\"+date_time+\"_%s\"%DETVAR_N_V[idx]+\"_%s\"%SELECTION+\"_\"+SAMPLEDEF+\".pdf\")\n",
    "    \n",
    "    #continue\n",
    "    \n",
    "    dfsubcommon = (dfsub.query(QUERY_CV)).query(QUERY_VAR)\n",
    "    \n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.hist2d(dfsubcommon['%s_CV'%VARIABLE],dfsubcommon['%s_VAR'%VARIABLE],bins=(BINS,BINS),norm=LogNorm())\n",
    "    plt.xlabel('%s [CV]'%TITLE)\n",
    "    plt.ylabel('%s [VAR]'%TITLE)\n",
    "    plt.title('%s'%DETVAR_N_V[idx])\n",
    "    plt.plot(BINS,BINS,'r--',lw=2)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    if (SAVEFIG):\n",
    "        fig.savefig(ls.main_path+\"plots/\"+VARIABLE+\"_corr_\"+date_time+\"_%s\"%DETVAR_N_V[idx]+\"_%s\"%SELECTION+\"_\"+SAMPLEDEF+\".pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "    #'''\n",
    "    BMAX = 1.0\n",
    "    BINS1D = np.linspace(-BMAX,BMAX,50)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    vals = (dfsubcommon['%s_CV'%VARIABLE].values - dfsubcommon['%s_VAR'%VARIABLE].values) / dfsubcommon['%s_CV'%VARIABLE].values\n",
    "    n,b,p = plt.hist(vals,bins=BINS1D,histtype='step',lw=2,normed=True)\n",
    "    b = 0.5*(b[1:]+b[:-1])\n",
    "    guess = [0.,0.05,10.]\n",
    "    popt,popv = curve_fit(gauss,b[10:-10],n[10:-10],p0=guess)\n",
    "    pope = np.sqrt(np.diag(popv))\n",
    "    plt.plot(BINS1D,gauss(BINS1D,*popt),'r--',lw=2,label=r'mu: %.03f sigma: %.03f'%(popt[0],popt[1]))\n",
    "    plt.xlim([-BMAX,BMAX])\n",
    "    plt.xlabel('%s [CV - VAR]/CV'%VARIABLE)\n",
    "    #plt.xlabel('%s [CV]'%TITLE)\n",
    "    #plt.ylabel('%s [VAR]'%TITLE)\n",
    "    plt.title('%s'%DETVAR_N_V[idx])\n",
    "    plt.legend(loc='best',fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    #plt.plot(BINS,BINS,'r--',lw=2)\n",
    "    if (SAVEFIG):\n",
    "        fig.savefig(ls.main_path+\"plots/\"+VARIABLE+\"_1d_\"+date_time+\"_%s\"%DETVAR_N_V[idx]+\"_%s\"%SELECTION+\"_\"+SAMPLEDEF+\".pdf\")\n",
    "    plt.show()\n",
    "    #'''\n",
    "    #continue\n",
    "    #print (frac_cov)\n",
    "            \n",
    "    fig, ax = plt.subplots(1, 1,figsize=(8,6))\n",
    "    pos = ax.imshow(frac_cov, origin='lower', cmap='viridis',vmin=-0.1,vmax=0.1)\n",
    "    ax.set_title(\"frac cov matrix : %s\"%DETVAR_N_V[idx])\n",
    "    ax.set_ylabel(\"Bin number\")\n",
    "    ax.set_xlabel(\"Bin number\")\n",
    "    # Add the text\n",
    "    # Limits for the extent\n",
    "    x_start = 0\n",
    "    x_end = len(n_cv)#-1\n",
    "    y_start = 0\n",
    "    y_end = len(n_cv)#-1\n",
    "    size = len(n_cv)#-1\n",
    "\n",
    "    x_positions = np.linspace(start=x_start, stop=x_end, num=size, endpoint=False)\n",
    "    y_positions = np.linspace(start=y_start, stop=y_end, num=size, endpoint=False)\n",
    "\n",
    "    for x_index, x in enumerate(x_positions):\n",
    "        #for x_index, x in enumerate(x_positions):\n",
    "        ERR = frac_cov[x_index, x_index]\n",
    "        label = \"%.01f\"%(100.*np.sqrt(ERR))\n",
    "        text_x = x #+ jump_x\n",
    "        text_y = x #+ jump_y\n",
    "        DIAG_VAL.append(np.sqrt(ERR))\n",
    "        # skip recombination variation\n",
    "        if (DETVAR_N_V[idx] != 'Recomb'):\n",
    "            DIAG_VAL_QUAD_V[x_index] += ERR\n",
    "\n",
    "        if (np.abs(ERR) > 0.05):\n",
    "            ax.text(text_x, text_y, label, color='black', ha='center', va='center',fontsize=8)\n",
    "        else:\n",
    "            ax.text(text_x, text_y, label, color='white', ha='center', va='center',fontsize=8)  \n",
    "    \n",
    "    print ('fractional error for var %s : \\n '%(DETVAR_N_V[idx]),DIAG_VAL)\n",
    "    DIAG_VAL_V_V.append(DIAG_VAL)\n",
    "\n",
    "    \n",
    "    fig.colorbar(pos)\n",
    "    if (SAVEFIG):\n",
    "        fig.savefig(ls.main_path+\"plots/\"+VARIABLE+\"_matrix_\"+date_time+\"_%s\"%DETVAR_N_V[idx]+\"_%s\"%SELECTION+\"_\"+SAMPLEDEF+\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_title(\"Correlation matrix\")\n",
    "    pos = ax.imshow(corr, origin='lower')\n",
    "    ax.set_ylabel(\"Bin number\")\n",
    "    ax.set_xlabel(\"Bin number\")\n",
    "    fig.colorbar(pos)\n",
    "    plt.show()\n",
    "    '''           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAG_FRAC_V = []\n",
    "for d in DIAG_VAL_QUAD_V:\n",
    "    DIAG_FRAC_V.append(np.sqrt(d))\n",
    "print ('quadrature sum of errors : ')\n",
    "print ('frac err: \\n',DIAG_FRAC_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open(ls.main_path+\"plots/\"+'detsys_'+VARIABLE+\"_\"+SELECTION+\".txt\",\"w\")\n",
    "strout = TITLE + ','\n",
    "for n in DETVAR_N_V:\n",
    "    strout += '%s,'%n\n",
    "strout += 'sum,'\n",
    "fout.write(strout+'\\n')\n",
    "for b in range(len(BINS)-1):\n",
    "    bmin = BINS[b]\n",
    "    bmax = BINS[b+1]\n",
    "    brange = '%.02f - %.02f'%(bmin,bmax)\n",
    "    strout = brange+','\n",
    "    for V_V in DIAG_VAL_V_V:\n",
    "        strout += '%.04f,'%(V_V[b])\n",
    "    strout += '%.04f,'%(DIAG_FRAC_V[b])\n",
    "    fout.write(strout+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "bcenter = 0.5*(BINS[:-1]+BINS[1:])\n",
    "\n",
    "for n,V_V in enumerate(DIAG_VAL_V_V):\n",
    "    \n",
    "    if (DETVAR_N_V[n] == \"Recomb\"):\n",
    "        continue\n",
    "    plt.step(bcenter,V_V,where='mid',label=DETVAR_N_V[n],lw=2)\n",
    "plt.step(bcenter,DIAG_FRAC_V,label='cumulative',color='k',lw=3,where='mid')    \n",
    "    \n",
    "plt.legend(loc=1,ncol=2,fontsize=14)\n",
    "\n",
    "plt.ylim([0,0.5])\n",
    "plt.xlabel(TITLE)\n",
    "plt.ylabel('Frac Uncertainty')\n",
    "plt.tight_layout()\n",
    "if (SAVEFIG):\n",
    "    plt.savefig(ls.main_path+\"plots/\"+\"_cumulative_detsys_\"+date_time+\"_%s\"%SELECTION+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAG_QUAD_V = np.zeros(len(DIAG_VAL_V[0]))\n",
    "\n",
    "for DIAG_VAL in DIAG_VAL_V:\n",
    "    DIAG_QUAD_V += (np.array(DIAG_VAL))**2\n",
    "\n",
    "\n",
    "    \n",
    "DIAG_QUAD_V = np.sqrt(DIAG_QUAD_V)\n",
    "print (100.*DIAG_QUAD_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eff(df,var,query,acceptance,bin_edges,absval=False):\n",
    "    #print acceptance\n",
    "    bin_centers = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "    bins = []\n",
    "    bin_eff = []\n",
    "    bin_err = []\n",
    "    for i in range(len(bin_centers)):\n",
    "        binmin = bin_edges[i]\n",
    "        binmax = bin_edges[i+1]\n",
    "        bincut = '%s > %f and %s < %f'%(var,binmin,var,binmax)\n",
    "        if (absval == True):\n",
    "            bincut = '(%s > %f and %s < %f) or (%s > -%f and %s < -%f)'%(var,binmin,var,binmax,var,binmax,var,binmin)\n",
    "        if (acceptance != ''): bincut += ' and %s'%acceptance\n",
    "        #print bincut\n",
    "        df_tmp =  df.query(bincut) # cut on bin range for desired var.\n",
    "        df_sub = df_tmp.query(query) # apply constrain \n",
    "        if (df_tmp.shape[0] == 0): continue\n",
    "        eff = df_sub.shape[0] / float( df_tmp.shape[0] )\n",
    "        err = np.sqrt( eff*(1-eff)/df_tmp.shape[0] )\n",
    "        bin_eff.append( eff )\n",
    "        bin_err.append( err )\n",
    "        bins.append(bin_centers[i])\n",
    "        #print 'eff = %.02f @ bin = %.02f'%(eff,bin_centers[i])\n",
    "    return np.array(bins),np.array(bin_eff),np.array(bin_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nue slection\n",
    "ACCEPTANCE_CV = 'isVtxInFiducial_CV == 1 and truthFiducial_CV == 1 and npi0_CV==0 and npion_CV==0 and nproton_CV>0 and selected_CV==1'\n",
    "ACCEPTANCE_VAR = 'isVtxInFiducial_VAR == 1 and truthFiducial_VAR == 1 and npi0_VAR==0 and npion_VAR==0 and nproton_VAR>0 and selected_VAR==1'\n",
    "\n",
    "#ACCEPTANCE_CV += ' and shr_bkt_pdg_CV == 11 and shr_bkt_purity_CV > 0.5'\n",
    "#ACCEPTANCE_VAR += ' and shr_bkt_pdg_VAR == 11 and shr_bkt_purity_VAR > 0.5'\n",
    "\n",
    "ACCEPTANCE_CV += ' and trk_bkt_pdg_CV == 2212 and trk_bkt_purity_CV > 0.5'\n",
    "ACCEPTANCE_VAR += ' and trk_bkt_pdg_VAR == 2212 and trk_bkt_purity_VAR > 0.5'\n",
    "\n",
    "QUERY_CV = NpBoxCutQuery('CV',BDT=False)\n",
    "QUERY_VAR = NpBoxCutQuery('VAR',BDT=False)\n",
    "\n",
    "ACCEPTANCE_CV += 'and %s'%QUERY_CV\n",
    "ACCEPTANCE_VAR += 'and %s'%QUERY_VAR\n",
    "\n",
    "print (ACCEPTANCE_CV)\n",
    "\n",
    "\n",
    "# pi0 slection\n",
    "#ACCEPTANCE_CV = 'isVtxInFiducial_CV == 1 and truthFiducial_CV == 1 and npi0_CV==1'\n",
    "#ACCEPTANCE_VAR = 'isVtxInFiducial_VAR == 1 and truthFiducial_VAR == 1 and npi0_VAR==1'\n",
    "\n",
    "# numu slection\n",
    "#ACCEPTANCE_CV = 'isVtxInFiducial_CV == 1 and truthFiducial_CV == 1 and ccnc_CV == 0'\n",
    "#ACCEPTANCE_VAR = 'isVtxInFiducial_VAR == 1 and truthFiducial_VAR == 1 and ccnc_VAR == 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY_CV = 'trkpid_CV < 0.02'\n",
    "#QUERY_VAR = 'trkpid_VAR < 0.02'\n",
    "\n",
    "#QUERY_CV = 'shr_tkfit_dedx_max_CV < 3.8'# and shr_tkfit_dedx_max_CV < 3.8'\n",
    "#QUERY_VAR = 'shr_tkfit_dedx_max_VAR < 3.8'# and shr_tkfit_dedx_max_VAR < 3.8'\n",
    "\n",
    "\n",
    "QUERY_CV = 'shr_tkfit_nhits_tot_CV > 1 and shr_tkfit_dedx_max_CV > 1.0 and shr_tkfit_dedx_max_CV < 3.8'\n",
    "QUERY_VAR = 'shr_tkfit_nhits_tot_VAR > 1 and shr_tkfit_dedx_max_VAR > 1.0 and shr_tkfit_dedx_max_VAR < 3.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "dfsub = (DETSYS_SAMPLE_V[0]).query(ACCEPTANCE_CV)\n",
    "plt.hist(dfsub['shr_tkfit_dedx_max_CV'].values,bins=np.linspace(0,10,51))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub = (DETSYS_SAMPLE_V[0]).query(ACCEPTANCE_CV)\n",
    "print (dfsub.shape[0])\n",
    "dfsub = dfsub.query(QUERY_CV)\n",
    "print (dfsub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#BINS = np.linspace(0.105,1.505,15)\n",
    "#BINS = np.linspace(0.,1.6,17)\n",
    "\n",
    "BINS = np.linspace(0.1,1.5,15)\n",
    "#BINS = np.linspace(0.938,1.438,15)\n",
    "\n",
    "\n",
    "print (BINS)\n",
    "#BINS = np.array([0.135,0.235,0.335,0.435,0.535,0.735,0.935,1.335])\n",
    "\n",
    "VARIABLE = 'elec_e'\n",
    "\n",
    "print (QUERY_CV)\n",
    "print ('...')\n",
    "print (ACCEPTANCE_CV)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize=(8,6))\n",
    "\n",
    "for i,df in enumerate(DETSYS_SAMPLE_V):\n",
    "    \n",
    "    if (i==0):\n",
    "        centers,vals,errs = Eff(df,VARIABLE+'_CV',QUERY_CV,ACCEPTANCE_CV,BINS)\n",
    "        #plt.plot(centers,vals,'ko-',label=r'CV')\n",
    "        plt.errorbar(centers,vals,xerr=(BINS[1]-BINS[0])/2.,fmt='o-',color='k',label=r'CV')\n",
    "        \n",
    "    #if (i > 2):\n",
    "    #    break\n",
    "    \n",
    "    centers,vals,errs = Eff(df,VARIABLE+'_VAR',QUERY_VAR,ACCEPTANCE_VAR,BINS)\n",
    "    #plt.errorbar(centers,vals,yerr=errs,fmt='o-',label=r'%s'%(DETVAR_N_V[i]))\n",
    "    plt.plot(centers,vals,'o--',label=r'%s'%(DETVAR_N_V[i]))\n",
    "\n",
    "plt.ylim([0.,0.7])    \n",
    "plt.legend(fontsize=16,loc=\"best\")\n",
    "    #plt.ylim([0,0.1])\n",
    "\n",
    "plt.ylabel('selection efficiency')\n",
    "plt.xlabel(r'true electron energy [GeV]')\n",
    "plt.title(r'$e$/$\\gamma$ d$E$/d$x$ cut [1.0, 3.8] MeV/cm')\n",
    "#plt.xlabel(r'true proton energy [GeV]')\n",
    "#plt.title(r'track PID cut [LLR-PID < 0.02]')\n",
    "plt.tight_layout()\n",
    "fig.savefig(ls.main_path+\"plots/\"+VARIABLE+\"_\"+date_time+\"_eff_shrpid_nues.pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Plotter.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
